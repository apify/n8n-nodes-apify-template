// without waitForFinish (waitForFinish = 0)
export const runActorResult = () => {
	return {
		data: {
			id: '5rsC83CHinQwPlsSI',
			actId: 'nFJndFXA5zjCTuudP',
			userId: 'A9zwKYff2yyRmaqc9',
			startedAt: '2025-06-30T12:36:08.502Z',
			finishedAt: null,
			status: 'READY',
			meta: {
				origin: 'API',
				userAgent: 'axios/1.7.4',
			},
			stats: {
				inputBodyLen: 346,
				migrationCount: 0,
				rebootCount: 0,
				restartCount: 0,
				resurrectCount: 0,
				computeUnits: 0,
			},
			options: {
				build: 'latest',
				timeoutSecs: 604800,
				memoryMbytes: 1024,
				maxTotalChargeUsd: 37.42362467983089,
				diskMbytes: 2048,
			},
			buildId: 'DgGC7ZxWmZ0cnuNIy',
			defaultKeyValueStoreId: 'dgt7oov3cthGsD4yq',
			defaultDatasetId: '63kMAihbWVgBvEAZ2',
			defaultRequestQueueId: 'cGohS4eFRrm2mItLx',
			pricingInfo: {
				pricingModel: 'PAY_PER_EVENT',
				reasonForChange:
					'We are introducing Store pricing discounts for this Actor and a new pricing model to give you more transparency and flexibility; more info in the follow-up email.',
				minimalMaxTotalChargeUsd: 0.5,
				createdAt: '2025-05-29T14:45:00.000Z',
				startedAt: '2025-06-10T08:00:00.000Z',
				apifyMarginPercentage: 0,
				notifiedAboutChangeAt: '2025-06-10T08:00:00.000Z',
				pricingPerEvent: {
					actorChargeEvents: {
						'actor-start': {
							eventTitle: 'Actor start',
							eventDescription: 'Flat fee for starting an Actor run.',
							eventPriceUsd: 0.0015,
						},
						'search-page-scraped': {
							eventTitle: 'Search results page scraped',
							eventDescription: 'Cost per page of Google Search results successfully scraped.',
							eventPriceUsd: 0.0035,
						},
						'ads-scraped': {
							eventTitle: 'Add-on: Paid results (ads) extraction',
							eventDescription:
								'Extra cost per page for attempting to extract paid results (ads) from Google Search. This applies when the ads extraction feature is enabled, regardless of whether ads are found on the specific page.',
							eventPriceUsd: 0.005,
						},
					},
				},
			},
			chargedEventCounts: {
				'actor-start': 0,
				'search-page-scraped': 0,
				'ads-scraped': 0,
			},
			platformUsageBillingModel: 'DEVELOPER',
			accountedChargedEventCounts: {
				'actor-start': 0,
				'search-page-scraped': 0,
				'ads-scraped': 0,
			},
			generalAccess: 'FOLLOW_USER_SETTING',
			buildNumber: '0.0.166',
			containerUrl: 'https://fttjdkagbv7c.runs.apify.net',
			usageTotalUsd: 0,
		},
	};
};

export const getSuccessRunResult = () => {
	return {
		data: {
			id: '5rsC83CHinQwPlsSI',
			actId: 'aYG0l9s7dbB7j3gbS',
			userId: 'jZRYihqGWNVbnW6gZ',
			startedAt: '2025-08-04T13:35:39.329Z',
			finishedAt: '2025-08-04T13:36:03.153Z',
			status: 'SUCCEEDED',
			statusMessage: 'Finished! Total 2 requests: 2 succeeded, 0 failed.',
			isStatusMessageTerminal: true,
			meta: {
				origin: 'API',
				userAgent: 'axios/1.8.3',
			},
			stats: {
				inputBodyLen: 1355,
				migrationCount: 0,
				rebootCount: 0,
				restartCount: 0,
				durationMillis: 23725,
				resurrectCount: 0,
				runTimeSecs: 23.725,
				metamorph: 0,
				computeUnits: 0.05272222222222222,
				memAvgBytes: 1271238550.168654,
				memMaxBytes: 1630343168,
				memCurrentBytes: 0,
				cpuAvgUsage: 122.24027931970461,
				cpuMaxUsage: 652.1327407407408,
				cpuCurrentUsage: 0,
				netRxBytes: 937113,
				netTxBytes: 148778,
			},
			options: {
				build: 'version-0',
				timeoutSecs: 360000,
				memoryMbytes: 8192,
				diskMbytes: 16384,
			},
			buildId: 'N5wC2ArUbLgbcEpaX',
			exitCode: 0,
			defaultKeyValueStoreId: 'PWz3wSqmi4xt1XFvF',
			defaultDatasetId: 'CBuuUCbfMENToBV07',
			defaultRequestQueueId: 'oUB2dER6vs2j1Gu2T',
			platformUsageBillingModel: 'USER',
			integrationTracking: {
				platform: 'n8n',
				appId: null,
			},
			generalAccess: 'FOLLOW_USER_SETTING',
			buildNumber: '0.3.67',
			containerUrl: 'https://h5v7gbfcrguy.runs.apify.net',
			usage: {
				ACTOR_COMPUTE_UNITS: 0.05272222222222222,
				DATASET_READS: 0,
				DATASET_WRITES: 1,
				KEY_VALUE_STORE_READS: 3,
				KEY_VALUE_STORE_WRITES: 11,
				KEY_VALUE_STORE_LISTS: 0,
				REQUEST_QUEUE_READS: 14,
				REQUEST_QUEUE_WRITES: 4,
				DATA_TRANSFER_INTERNAL_GBYTES: 0.0009098071604967117,
				DATA_TRANSFER_EXTERNAL_GBYTES: 0.000056576915085315704,
				PROXY_RESIDENTIAL_TRANSFER_GBYTES: 0,
				PROXY_SERPS: 0,
			},
			usageTotalUsd: 0.02185169462993079,
			usageUsd: {
				ACTOR_COMPUTE_UNITS: 0.02108888888888889,
				DATASET_READS: 0,
				DATASET_WRITES: 0.000005,
				KEY_VALUE_STORE_READS: 0.000015000000000000002,
				KEY_VALUE_STORE_WRITES: 0.00055,
				KEY_VALUE_STORE_LISTS: 0,
				REQUEST_QUEUE_READS: 0.000056,
				REQUEST_QUEUE_WRITES: 0.00008,
				DATA_TRANSFER_INTERNAL_GBYTES: 0.00004549035802483559,
				DATA_TRANSFER_EXTERNAL_GBYTES: 0.000011315383017063142,
				PROXY_RESIDENTIAL_TRANSFER_GBYTES: 0,
				PROXY_SERPS: 0,
			},
			consoleUrl: 'https://console.apify.com/view/runs/5rsC83CHinQwPlsSI',
		},
	};
};

export const getActorResult = () => {
	return {
		data: {
			id: 'aYG0l9s7dbB7j3gbS',
			userId: 'ZscMwFR5H7eCtWtyh',
			name: 'website-content-crawler',
			username: 'apify',
			description:
				'Crawl websites and extract text content to feed AI models, LLM applications, vector databases, or RAG pipelines. The Actor supports rich formatting using Markdown, cleans the HTML, downloads files, and integrates well with ðŸ¦œðŸ”— LangChain, LlamaIndex, and the wider LLM ecosystem.',
			restartOnError: true,
			isPublic: true,
			createdAt: '2023-03-27T15:06:04.992Z',
			modifiedAt: '2025-07-30T15:03:12.367Z',
			stats: {
				totalBuilds: 334,
				totalRuns: 16551896,
				totalUsers: 65794,
				totalUsers7Days: 2840,
				totalUsers30Days: 8467,
				totalUsers90Days: 20761,
				lastRunStartedAt: '2025-08-04T14:33:25.134Z',
				totalMetamorphs: 476,
				publicActorRunStats30Days: {
					ABORTED: 7000,
					FAILED: 4488,
					SUCCEEDED: 1781763,
					'TIMED-OUT': 48709,
					TOTAL: 1843797,
				},
			},
			versions: [],
			defaultRunOptions: {
				build: 'version-0',
				timeoutSecs: 360000,
				memoryMbytes: 8192,
			},
			exampleRunInput: {
				body: '{ "helloWorld": 123 }',
				contentType: 'application/json; charset=utf-8',
			},
			categories: ['AI', 'DEVELOPER_TOOLS'],
			isDeprecated: false,
			title: 'Website Content Crawler',
			pictureUrl:
				'https://apify-image-uploads-prod.s3.amazonaws.com/aYG0l9s7dbB7j3gbS/PfToENkJZxahzPDu3-CleanShot_2023-03-28_at_10.40.20_2x.png',
			seoTitle: null,
			seoDescription: null,
			notice: 'NONE',
			isCritical: true,
			isGeneric: false,
			hasNoDataset: false,
			isSourceCodeHidden: true,
			standbyUrl: null,
			actorPermissionLevel: 'FULL_PERMISSIONS',
			deploymentKey:
				'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCe3jwR6yJ3C2l7b+V4hx4X5s+Z3XFrGmau9Yy0GC82VkTDpCKgEflGqdaLJIFmeWT1UHDwOIrCWSwk4W4DEVYQwJE3sgkoUFBGbBo0opLc/vyZFSnvjyleUwGt0vxZg2EyEgH1i9+aRirUHVNAat92Au1viClnEih1lffP0WyN4VuVgrQO0Qf+UcPfq87Adeb9UK9ZbwcWx/yhnUMwIHbWnohshyWPCWJPfiDEUFcdmFs3JfLlQZJ6y79g9zw3AFppJEIeVJhK3bHIa4mfLND049SWJxhR/prvLrCk6ZbOI2D19/CeYJNmCGxSBjPKeohIFwBvShnQZhkXIN5YkZAz \n',
			taggedBuilds: {
				beta: {
					buildId: 'IfgOtMS3MoEtjd6I4',
					buildNumber: '0.1.198',
					finishedAt: '2025-07-30T15:03:12.367Z',
				},
				'version-0': {
					buildId: 'N5wC2ArUbLgbcEpaX',
					buildNumber: '0.3.67',
					finishedAt: '2025-06-19T08:28:08.898Z',
				},
				canary: {
					buildId: '4couMa0WqADhfhqwT',
					buildNumber: '0.2.44',
					finishedAt: '2025-07-17T14:03:15.319Z',
				},
			},
		},
	};
};

export const getBuildResult = () => {
	return {
		data: {
			id: 'N5wC2ArUbLgbcEpaX',
			actId: 'aYG0l9s7dbB7j3gbS',
			userId: 'ZscMwFR5H7eCtWtyh',
			startedAt: '2025-06-19T08:25:44.145Z',
			finishedAt: '2025-06-19T08:28:08.898Z',
			status: 'SUCCEEDED',
			meta: {
				origin: 'CLI',
				userAgent: 'ApifyClient/2.12.5 (Linux; Node/v20.19.2); isAtHome/false',
			},
			stats: {
				durationMillis: 144638,
				runTimeSecs: 144.638,
				computeUnits: 0.1607088888888889,
				imageSizeBytes: 1492126502,
			},
			options: {
				useCache: true,
				betaPackages: false,
				memoryMbytes: 4096,
				diskMbytes: 8192,
			},
			exitCode: 0,
			inputSchema:
				'{\n  "title": "Input schema for Website Content Crawler",\n  "description": "Enter **Start URLs** of websites to crawl, ensure you are using the right **Crawler type**, check the other optional settings, and click **Start** to run the crawler.",\n  "type": "object",\n  "schemaVersion": 1,\n  "properties": {\n    "startUrls": {\n      "title": "Start URLs",\n      "type": "array",\n      "description": "One or more URLs of pages where the crawler will start.\\n\\nBy default, the Actor will also crawl sub-pages of these URLs. For example, for start URL `https://example.com/blog`, it will crawl also `https://example.com/blog/post` or `https://example.com/blog/article`. The **Include URLs (globs)** option overrides this automation behavior.",\n      "editor": "requestListSources",\n      "prefill": [\n        {\n          "url": "https://docs.apify.com/academy/web-scraping-for-beginners"\n        }\n      ]\n    },\n    "useSitemaps": {\n      "title": "Load URLs from Sitemaps",\n      "type": "boolean",\n      "description": "If enabled, the crawler will look for [Sitemaps](https://en.wikipedia.org/wiki/Sitemaps) at the domains of the provided *Start URLs* and enqueue matching URLs similarly as the links found on crawled pages. You can also reference a `sitemap.xml` file directly by adding it as another Start URL (e.g. `https://www.example.com/sitemap.xml`)\\n\\nThis feature makes the crawling more robust on websites that support Sitemaps, as it includes pages that might be not reachable from Start URLs. However, **loading and processing Sitemaps can take a lot of time, especially for large sites**. Note that if a page is found via Sitemaps, it will have `depth` of `1`.",\n      "default": false,\n      "prefill": false\n    },\n    "respectRobotsTxtFile": {\n      "title": "Respect the robots.txt file",\n      "type": "boolean",\n      "description": "If enabled, the crawler will consult the robots.txt file for the target website before crawling each page. At the moment, the crawler does not use any specific user agent identifier. The crawl-delay directive is also not supported yet.",\n      "default": false,\n      "prefill": true\n    },\n    "crawlerType": {\n      "title": "Crawler type",\n      "type": "string",\n      "enum": [\n        "playwright:adaptive",\n        "playwright:firefox",\n        "cheerio",\n        "jsdom",\n        "playwright:chrome"\n      ],\n      "enumTitles": [\n        "Adaptive switching between browser and raw HTTP - Fast and renders JavaScript content if present. This is the recommended option.",\n        "Headless browser (Firefox+Playwright) - Reliable, renders JavaScript content, best in avoiding blocking, but might be slow.",\n        "Raw HTTP client (Cheerio) - Fastest, but doesn\'t render JavaScript content.",\n        "Raw HTTP client with JavaScript (JSDOM) - Experimental, use at your own risk.",\n        "[DEPRECATED] Headless browser (Chrome+Playwright) - The crawler will use Firefox+Playwright instead."\n      ],\n      "description": "Select the crawling engine:\\n- **Headless web browser** - Useful for modern websites with anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it requires more computing resources and is slower. It is recommended to use at least 8 GB of RAM.\\n- **Stealthy web browser** (default) - Another headless web browser with anti-blocking measures enabled. Try this if you encounter bot protection while scraping. For best performance, use with Apify Proxy residential IPs. \\n- **Adaptive switching between Chrome and raw HTTP client** - The crawler automatically switches between raw HTTP for static pages and Chrome browser (via Playwright) for dynamic pages, to get the maximum performance wherever possible. \\n- **Raw HTTP client** - High-performance crawling mode that uses raw HTTP requests to fetch the pages. It is faster and cheaper, but it might not work on all websites.\\n\\nBeware that with the raw HTTP client or adaptive crawling mode, some features are not available, e.g. wait for dynamic content, maximum scroll height, or remove cookie warnings.",\n      "default": "playwright:firefox",\n      "prefill": "playwright:adaptive"\n    },\n    "includeUrlGlobs": {\n      "sectionCaption": "Crawler settings",\n      "title": "Include URLs (globs)",\n      "type": "array",\n      "description": "Glob patterns matching URLs of pages that will be included in crawling. \\n\\nSetting this option will disable the default Start URLs based scoping and will allow you to customize the crawling scope yourself. Note that this affects only links found on pages, but not **Start URLs** - if you want to crawl a page, make sure to specify its URL in the **Start URLs** field. \\n\\nFor example `https://{store,docs}.example.com/**` lets the crawler to access all URLs starting with `https://store.example.com/` or `https://docs.example.com/`, and `https://example.com/**/*\\\\?*foo=*` allows the crawler to access all URLs that contain `foo` query parameter with any value.\\n\\nLearn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fscrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fscrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fscrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this).",\n      "editor": "globs",\n      "prefill": [],\n      "default": []\n    },\n    "excludeUrlGlobs": {\n      "title": "Exclude URLs (globs)",\n      "type": "array",\n      "description": "Glob patterns matching URLs of pages that will be excluded from crawling. Note that this affects only links found on pages, but not **Start URLs**, which are always crawled. \\n\\nFor example `https://{store,docs}.example.com/**` excludes all URLs starting with `https://store.example.com/` or `https://docs.example.com/`, and `https://example.com/**/*\\\\?*foo=*` excludes all URLs that contain `foo` query parameter with any value.\\n\\nLearn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fscrape_this).",\n      "editor": "globs",\n      "prefill": [],\n      "default": []\n    },\n    "keepUrlFragments": {\n      "title": "URL #fragments identify unique pages",\n      "type": "boolean",\n      "description": "Indicates that URL fragments (e.g. <code>http://example.com<b>#fragment</b></code>) should be included when checking whether a URL has already been visited or not. Typically, URL fragments are used for page navigation only and therefore they should be ignored, as they don\'t identify separate pages. However, some single-page websites use URL fragments to display different pages; in such a case, this option should be enabled.",\n      "default": false\n    },\n    "ignoreCanonicalUrl": {\n      "title": "Ignore canonical URLs",\n      "type": "boolean",\n      "description": "If enabled, the Actor will ignore the canonical URL reported by the page, and use the actual URL instead. You can use this feature for websites that report invalid canonical URLs, which causes the Actor to skip those pages in results.",\n      "default": false\n    },\n    "ignoreHttpsErrors": {\n      "title": "Ignore HTTPS errors",\n      "type": "boolean",\n      "description": "If enabled, the scraper will ignore HTTPS certificate errors. Use at your own risk.",\n      "default": false\n    },\n    "maxCrawlDepth": {\n      "title": "Max crawling depth",\n      "type": "integer",\n      "description": "The maximum number of links starting from the start URL that the crawler will recursively follow. The start URLs have depth `0`, the pages linked directly from the start URLs have depth `1`, and so on.\\n\\nThis setting is useful to prevent accidental crawler runaway. By setting it to `0`, the Actor will only crawl the Start URLs.",\n      "minimum": 0,\n      "default": 20\n    },\n    "maxCrawlPages": {\n      "title": "Max pages",\n      "type": "integer",\n      "description": "The maximum number pages to crawl. It includes the start URLs, pagination pages, pages with no content, etc. The crawler will automatically finish after reaching this number. This setting is useful to prevent accidental crawler runaway.",\n      "minimum": 0,\n      "default": 9999999\n    },\n    "initialConcurrency": {\n      "title": "Initial concurrency",\n      "type": "integer",\n      "description": "The initial number of web browsers or HTTP clients running in parallel. The system scales the concurrency up and down based on the current CPU and memory load. If the value is set to 0 (default), the Actor uses the default setting for the specific crawler type.\\n\\nNote that if you set this value too high, the Actor will run out of memory and crash. If too low, it will be slow at start before it scales the concurrency up.",\n      "minimum": 0,\n      "maximum": 999,\n      "default": 0\n    },\n    "maxConcurrency": {\n      "title": "Max concurrency",\n      "type": "integer",\n      "description": "The maximum number of web browsers or HTTP clients running in parallel. This setting is useful to avoid overloading the target websites and to avoid getting blocked.",\n      "minimum": 1,\n      "maximum": 999,\n      "default": 200\n    },\n    "initialCookies": {\n      "title": "Initial cookies",\n      "type": "array",\n      "description": "Cookies that will be pre-set to all pages the scraper opens. This is useful for pages that require login. The value is expected to be a JSON array of objects with `name` and `value` properties. For example: `[{\\"name\\": \\"cookieName\\", \\"value\\": \\"cookieValue\\"}]`.\\n\\nYou can use the [EditThisCookie](https://chrome.google.com/webstore/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg) browser extension to copy browser cookies in this format, and paste it here.",\n      "default": [],\n      "prefill": [],\n      "editor": "json"\n    },\n    "proxyConfiguration": {\n      "title": "Proxy configuration",\n      "type": "object",\n      "description": "Enables loading the websites from IP addresses in specific geographies and to circumvent blocking.",\n      "default": {\n        "useApifyProxy": true\n      },\n      "prefill": {\n        "useApifyProxy": true\n      },\n      "editor": "proxy"\n    },\n    "maxSessionRotations": {\n      "title": "Maximum number of session rotations",\n      "type": "integer",\n      "description": "The maximum number of times the crawler will rotate the session (IP address + browser configuration) on anti-scraping measures like CAPTCHAs. If the crawler rotates the session more than this number and the page is still blocked, it will finish with an error.",\n      "minimum": 0,\n      "maximum": 20,\n      "default": 10\n    },\n    "maxRequestRetries": {\n      "title": "Maximum number of retries on network / server errors",\n      "type": "integer",\n      "description": "The maximum number of times the crawler will retry the request on network, proxy or server errors. If the (n+1)-th request still fails, the crawler will mark this request as failed.",\n      "minimum": 0,\n      "maximum": 20,\n      "default": 3\n    },\n    "requestTimeoutSecs": {\n      "title": "Request timeout",\n      "type": "integer",\n      "description": "Timeout in seconds for making the request and processing its response. Defaults to 60s.",\n      "minimum": 1,\n      "maximum": 600,\n      "default": 60,\n      "unit": "seconds"\n    },\n    "minFileDownloadSpeedKBps": {\n      "title": "Minimum file download speed",\n      "type": "integer",\n      "description": "The minimum viable file download speed in kilobytes per seconds. If the file download speed is lower than this value for a prolonged duration, the crawler will consider the file download as failing, abort it, and retry it again (up to \\"Maximum number of retries\\" times). This is useful to avoid your crawls being stuck on slow file downloads.",\n      "default": 128,\n      "unit": "kilobytes per second"\n    },\n    "dynamicContentWaitSecs": {\n      "sectionCaption": "HTML processing",\n      "title": "Wait for dynamic content",\n      "type": "integer",\n      "description": "The maximum time in seconds to wait for dynamic page content to load. By default, it is 10 seconds. The crawler will continue processing the page either if this time elapses, or if it detects the network became idle as there are no more requests for additional resources.\\n\\nWhen using the **Wait for selector** option, the crawler will wait for the selector to appear for this amount of time. If the selector doesn\'t appear within this period, the request will fail and will be retried.\\n\\nNote that this setting is ignored for the raw HTTP client, because it doesn\'t execute JavaScript or loads any dynamic resources. Similarly, if the value is set to `0`, the crawler doesn\'t wait for any dynamic to load and processes the HTML as provided on load.",\n      "default": 10,\n      "unit": "seconds"\n    },\n    "waitForSelector": {\n      "title": "Wait for selector",\n      "type": "string",\n      "description": "If set, the crawler will wait for the specified CSS selector to appear in the page before proceeding with the content extraction. This is useful for pages for which the default content load recognition by idle network fails. Setting this option completely disables the default behavior, and the page will be processed only if the element specified by this selector appears. If the element doesn\'t appear within the **Wait for dynamic content** timeout, the request will fail and will be retried later. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function.\\n\\nWith the raw HTTP client, this option checks for the presence of the selector in the HTML content and throws an error if it\'s not found.",\n      "default": "",\n      "editor": "textfield"\n    },\n    "softWaitForSelector": {\n      "title": "Soft wait for selector",\n      "type": "string",\n      "description": "If set, the crawler will wait for the specified CSS selector to appear in the page before proceeding with the content extraction. Unlike the `waitForSelector` option, this option doesn\'t fail the request if the selector doesn\'t appear within the timeout (the request processing will continue).",\n      "default": "",\n      "editor": "textfield"\n    },\n    "maxScrollHeightPixels": {\n      "title": "Maximum scroll height",\n      "type": "integer",\n      "minimum": 0,\n      "description": "The crawler will scroll down the page until all content is loaded (and network becomes idle), or until this maximum scrolling height is reached. Setting this value to `0` disables scrolling altogether.\\n\\nNote that this setting is ignored for the raw HTTP client, because it doesn\'t execute JavaScript or loads any dynamic resources.",\n      "default": 5000,\n      "unit": "pixels"\n    },\n    "keepElementsCssSelector": {\n      "title": "Keep HTML elements (CSS selector)",\n      "type": "string",\n      "description": "An optional CSS selector matching HTML elements that should be preserved in the DOM. If provided, all HTML elements which are not matching the CSS selectors or their descendants are removed from the DOM. This is useful to extract only relevant page content. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function. \\n\\nThis option runs before the `HTML transformer` option. If you are missing content in the output despite using this option, try disabling the `HTML transformer`.",\n      "editor": "textarea",\n      "default": "",\n      "prefill": ""\n    },\n    "removeElementsCssSelector": {\n      "title": "Remove HTML elements (CSS selector)",\n      "type": "string",\n      "description": "A CSS selector matching HTML elements that will be removed from the DOM, before converting it to text, Markdown, or saving as HTML. This is useful to skip irrelevant page content. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function. \\n\\nBy default, the Actor removes common navigation elements, headers, footers, modals, scripts, and inline image. You can disable the removal by setting this value to some non-existent CSS selector like `dummy_keep_everything`.",\n      "editor": "textarea",\n      "default": "nav, footer, script, style, noscript, svg, img[src^=\'data:\'],\\n[role=\\"alert\\"],\\n[role=\\"banner\\"],\\n[role=\\"dialog\\"],\\n[role=\\"alertdialog\\"],\\n[role=\\"region\\"][aria-label*=\\"skip\\" i],\\n[aria-modal=\\"true\\"]",\n      "prefill": "nav, footer, script, style, noscript, svg, img[src^=\'data:\'],\\n[role=\\"alert\\"],\\n[role=\\"banner\\"],\\n[role=\\"dialog\\"],\\n[role=\\"alertdialog\\"],\\n[role=\\"region\\"][aria-label*=\\"skip\\" i],\\n[aria-modal=\\"true\\"]"\n    },\n    "removeCookieWarnings": {\n      "title": "Remove cookie warnings",\n      "type": "boolean",\n      "description": "If enabled, the Actor will try to remove cookies consent dialogs or modals, using the [I don\'t care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension, to improve the accuracy of the extracted text. Note that there is a small performance penalty if this feature is enabled.\\n\\nThis setting is ignored when using the raw HTTP crawler type.",\n      "default": true\n    },\n    "blockMedia": {\n      "title": "Block loading of images and videos",\n      "type": "boolean",\n      "description": "If the flag is enabled and the Actor is using a headless browser, it will not load images, fonts, stylesheets and videos to improve performance. It will load scripts as usual - that is after all the point of using a headless browser.",\n      "default": false,\n      "prefill": true\n    },\n    "expandIframes": {\n      "title": "Expand iframe elements",\n      "type": "boolean",\n      "description": "By default, the Actor will extract content from `iframe` elements. If you want to specifically skip `iframe` processing, disable this option. Works only for the `playwright:firefox` crawler type.",\n      "default": true\n    },\n    "clickElementsCssSelector": {\n      "title": "Expand clickable elements",\n      "type": "string",\n      "editor": "textfield",\n      "description": "A CSS selector matching DOM elements that will be clicked. This is useful for expanding collapsed sections, in order to capture their text content. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function. ",\n      "default": "[aria-expanded=\\"false\\"]",\n      "prefill": "[aria-expanded=\\"false\\"]"\n    },\n    "stickyContainerCssSelector": {\n      "title": "Make containers sticky",\n      "type": "string",\n      "editor": "textfield",\n      "description": "This is an **experimental** feature. A CSS selector matching DOM elements that will be prevented from deleting any of their children. This is useful in conjunction with the \\"Expand clickable elements\\" option on pages where hidden content is actually removed from the DOM (i.e., some variants of the accordion pattern). Enabling this might corrupt the extracted content, which is why it is disabled by default. It is possible to enable the feature for the whole page with the `*` selector, or you can target specific elements if the former has unwanted side effects."\n    },\n    "htmlTransformer": {\n      "title": "HTML transformer",\n      "type": "string",\n      "enum": [\n        "readableTextIfPossible",\n        "readableText",\n        "extractus",\n        "none"\n      ],\n      "enumTitles": [\n        "Mozilla Readability with fallback",\n        "Mozilla Readability",\n        "Extractus",\n        "None"\n      ],\n      "description": "Specify how to transform the HTML to extract meaningful content without any extra fluff, like navigation or modals. The HTML transformation happens after removing and clicking the DOM elements.\\n\\n- **Readable text with fallback** - Uses Mozilla\'s Readability to extract the main content but falls back to the original HTML if the page doesn\'t appear to be an article. This is useful for websites with mixed content types (articles, product pages, etc.) as it preserves more content on non-article pages.\\n\\n- **Readable text** (default) - Uses Mozilla\'s Readability to extract the main article content, removing navigation, headers, footers, and other non-essential elements. Works best for article-rich websites and blogs.\\n\\n- **Extractus** - Uses the Extractus article extraction library, which is an alternative content extraction algorithm. May work better than Readability on certain websites, particularly news sites or blogs with specific layouts.\\n\\n- **None** - Only applies basic cleaning (removing elements specified via \'Remove HTML elements\' option) without any content extraction algorithm. Best when you want to preserve most of the original HTML structure with minimal processing.\\n\\nYou can examine output of all transformers by enabling the debug mode.\\n",\n      "default": "readableText"\n    },\n    "readableTextCharThreshold": {\n      "title": "Readable text extractor character threshold",\n      "type": "integer",\n      "description": "A configuration options for the \\"Readable text\\" HTML transformer. It contains the minimum number of characters an article must have in order to be considered relevant.",\n      "default": 100,\n      "editor": "hidden"\n    },\n    "aggressivePrune": {\n      "title": "Remove duplicate text lines",\n      "type": "boolean",\n      "description": "This is an **experimental feature**. If enabled, the crawler will prune content lines that are very similar to the ones already crawled on other pages, using the Count-Min Sketch algorithm. This is useful to strip repeating content in the scraped data like menus, headers, footers, etc. In some (not very likely) cases, it might remove relevant content from some pages.",\n      "default": false\n    },\n    "debugMode": {\n      "title": "Debug mode (stores output of all HTML transformers)",\n      "type": "boolean",\n      "description": "If enabled, the Actor will store the output of all types of HTML transformers, including the ones that are not used by default, and it will also store the HTML to Key-value Store with a link. All this data is stored under the `debug` field in the resulting Dataset.",\n      "default": false\n    },\n    "debugLog": {\n      "title": "Debug log",\n      "type": "boolean",\n      "description": "If enabled, the actor log will include debug messages. Beware that this can be quite verbose.",\n      "default": false\n    },\n    "saveHtml": {\n      "sectionCaption": "Output settings",\n      "title": "Save HTML to dataset (deprecated)",\n      "type": "boolean",\n      "description": "If enabled, the crawler stores full transformed HTML of all pages found to the output dataset under the `html` field. **This option has been deprecated** in favor of the `saveHtmlAsFile` option, because the dataset records have a size of approximately 10MB and it\'s harder to review the HTML for debugging.",\n      "default": false\n    },\n    "saveHtmlAsFile": {\n      "title": "Save HTML to key-value store",\n      "type": "boolean",\n      "description": "If enabled, the crawler stores full transformed HTML of all pages found to the default key-value store and saves links to the files as `htmlUrl` field in the output dataset. Storing HTML in key-value store is preferred to storing it into the dataset with the `saveHtml` option, because there\'s no size limit and it\'s easier for debugging as you can easily view the HTML.",\n      "default": false\n    },\n    "saveMarkdown": {\n      "title": "Save Markdown",\n      "type": "boolean",\n      "description": "If enabled, the crawler converts the transformed HTML of all pages found to Markdown, and stores it under the `markdown` field in the output dataset.",\n      "default": true\n    },\n    "saveFiles": {\n      "title": "Save files",\n      "type": "boolean",\n      "description": "If enabled, the crawler downloads files linked from the web pages, as long as their URL has one of the following file extensions: PDF, DOC, DOCX, XLS, XLSX, and CSV. Note that unlike web pages, the files are downloaded regardless if they are under **Start URLs** or not. The files are stored to the default key-value store, and metadata about them to the output dataset, similarly as for web pages.",\n      "default": false\n    },\n    "saveScreenshots": {\n      "title": "Save screenshots (headless browser only)",\n      "type": "boolean",\n      "description": "If enabled, the crawler stores a screenshot for each article page to the default key-value store. The link to the screenshot is stored under the `screenshotUrl` field in the output dataset. It is useful for debugging, but reduces performance and increases storage costs.\\n\\nNote that this feature only works with the `playwright:firefox` crawler type.",\n      "default": false\n    },\n    "maxResults": {\n      "title": "Max results",\n      "type": "integer",\n      "description": "The maximum number of resulting web pages to store. The crawler will automatically finish after reaching this number. This setting is useful to prevent accidental crawler runaway. If both **Max pages** and **Max results** are defined, then the crawler will finish when the first limit is reached. Note that the crawler skips pages with the canonical URL of a page that has already been crawled, hence it might crawl more pages than there are results.",\n      "minimum": 0,\n      "default": 9999999\n    },\n    "textExtractor": {\n      "title": "Text extractor (deprecated)",\n      "type": "string",\n      "description": "Removed in favor of the `htmlTransformer` option. Will be removed soon.",\n      "editor": "hidden"\n    },\n    "clientSideMinChangePercentage": {\n      "title": "(Adaptive crawling only) Minimum client-side content change percentage",\n      "description": "The least amount of content (as a percentage) change after the initial load required to consider the pages client-side rendered",\n      "type": "integer",\n      "minimum": 1,\n      "editor": "hidden",\n      "default": 15\n    },\n    "renderingTypeDetectionPercentage": {\n      "title": "(Adaptive crawling only) How often should the crawler attempt to detect page rendering type",\n      "description": "How often should the adaptive attempt to detect page rendering type",\n      "type": "integer",\n      "minimum": 1,\n      "maximum": 100,\n      "editor": "hidden",\n      "default": 10\n    }\n  },\n  "required": [\n    "startUrls",\n    "proxyConfiguration"\n  ]\n}',
			readme:
				'Website Content Crawler is an [Apify Actor](https://docs.apify.com/platform/actors) that can perform\na deep crawl of one or more websites and extract text content from the web pages.\nIt is useful to download data from websites such as documentation,\nknowledge bases, help sites, or blogs.\n\nThe Actor was specifically designed to extract data for feeding, fine-tuning, or training\nlarge language models (LLMs) such as [GPT-4](https://openai.com/product/gpt-4), [ChatGPT](https://openai.com/blog/chatgpt),\nor [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/).\n\nWebsite Content Crawler has a simple input configuration so that it can be easily integrated into customer-facing products, where customers\ncan enter just a URL of the website they want to have indexed by an AI application.\nYou can retrieve the results using the API to formats such as JSON or CSV,\nwhich can be fed directly to your LLM,\n[vector database](https://blog.apify.com/what-is-a-vector-database/), or directly to ChatGPT.\n\n\n## Main features\n\nWebsite Content Crawler is built upon [Crawlee](https://crawlee.dev/), Apify\'s state-of-the-art\nopen-source library for web scraping and crawling. The Actor can:\n\n- Crawl JavaScript-enabled websites using **headless Firefox** or simple sites using **raw HTTP**.\n- Circumvent **anti-scraping protections** using browser fingerprinting and proxies.\n- Save web content in plain text, **Markdown**, or HTML.\n- Crawl **pages behind a login**.\n- **Download files** in PDF, DOC, DOCX, XLS, XLSX, or CSV formats.\n- **Remove fluff** from pages like navigation, header, footers, ads, modals, or cookies warnings to improve the accuracy of the data.\n- Load content of pages with **infinite scroll**.\n- Use sitemaps to find more URLs on the website.\n- **Scale gracefully** from tiny sites to sites with millions of pages by leveraging the Apify platform capabilities.\n- Integrate with **ðŸ¦œðŸ”—LangChain**,: **LlamaIndex**, **Haystack**, **Pinecone**, **Qdrant**, or **OpenAI Assistant**\n- and much more...\n\nLearn about the key features and capabilities in the **Website Content Crawler Overview** video:\n\n[Introducing Website Content Crawler](https://www.youtube.com/watch?v=vUMPfIOfXXQ)\n\nStill unsure if the Website Content Crawler can handle your use case? Simply try it for free and see the results for yourself.\n\n\n## Designed for generative AI and LLMs\n\nThe results of Website Content Crawler can help you feed, fine-tune or train your large language models (LLMs)\nor provide context for prompts for ChatGPT.\nIn return, the model will answer questions based on your or your customer\'s websites and content.\n\nTo learn more, check out our **Web Scraping Data for Generative AI** video on this topic, showcasing the Website Content Crawler:\n\n[Web Scraping Data for Generative AI webinar](https://www.youtube.com/watch?v=8uvHH-ocSes)\n\n**Custom chatbots for customer support**\n\nCustomer service chatbots personalized on customer websites, such as documentation or knowledge bases,\nare one of the most promising use cases of AI and LLMs. Let your\ncustomers easily onboard by typing the URL of their site, and thus give your chatbot detailed\nknowledge of their product or service.\nLearn more about this use case in our [blog post](https://blog.apify.com/talk-to-your-website-with-large-language-models/).\n\n**Generate personalized content based on customerâ€™s copy**\n\nChatGPT and LLMs can write articles for you, but they wonâ€™t sound like you wrote them. Feed all your old blogs into your\nmodel to make it sound like you. Alternatively, train the model on your customersâ€™ blogs and have it write in their tone of voice.\nOr help their technical writers with making first drafts of new documentation pages.\n\n**Retrieval Augmented Generation (RAG) use cases**\n\nUse your website content to create an all-knowing AI assistant. The LLM-powered bot can then answer questions based on your website content,\nor even generate new content based on the existing one. This is a great way to provide a personalized experience to your customers\nor help your employees find the information they need faster.\n\n**Summarization, translation, proofreading at scale**\n\nGot some old docs or blogs that need to be improved? Use Website Content Crawler to scrape the content, feed it to the ChatGPT API,\nand ask it to summarize, proofread, translate, or change the style of the content.\n\n**Enhance your custom GPTs**\n\nUploading knowledge files gives custom OpenAI GPTs reliable information to refer to when generating answers. With Website Content Crawler, you can scrape data from any website to [provide your GPT with custom knowledge](https://blog.apify.com/custom-gpts-knowledge/).\n\n\n## How does it work?\n\nWebsite Content Crawler operates in three stages:\n\n1) **Crawling** - Finds and downloads the right web pages.\n2) **HTML processing** - Transforms the DOM of crawled pages to e.g. remove navigation, header, footer, cookie warnings, and other fluff.\n3) **Output** - Converts the resulting DOM to plain text or Markdown and saves downloaded files.\n\nFor clarity, the input settings of the Actor are organized according to the above stages. Note that input settings\nhave reasonable defaultsâ€”the only mandatory setting is the **Start URLs**.\n\n### Crawling\n\nWebsite Content Crawler only needs one or more **Start URLs** to run, typically the top-level URL of the documentation site, blog, or\nknowledge base that you want to scrape. The actor crawls the start URLs, finds links to other pages,\nand recursively crawls those pages, too, as long as their URL is under the start URL.\n\nFor example, if you enter the start URL `https://example.com/blog/`, the\nactor will crawl pages like `https://example.com/blog/article-1` or `https://example.com/blog/section/article-2`,\nbut will skip pages like `https://example.com/docs/something-else`.\n\nYou can also force the crawler to skip certain URLs using the **Exclude URLs (globs)** input setting,\nwhich specifies an array of glob patterns matching URLs of pages to be skipped.\nNote that this setting affects only links found on pages, but not **Start URLs**, which are always crawled.\nFor example, `https://{store,docs}.example.com/**` will exclude all URLs starting with\n`https://store.example.com/` and `https://docs.example.com/`.\nOr `https://example.com/**/*\\?*foo=*` exclude all URLs that contain `foo` query parameter with any value.\nYou can learn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fscrape_this).\n\nThe Actor automatically skips duplicate pages identified by the same [canonical URL](https://en.wikipedia.org/wiki/Canonical_link_element);\nthose pages are loaded and counted towards the _Max pages_ limit but not saved to the results.\n\nWebsite Content Crawler provides various input settings to customize the crawling.\nFor example, you can select the crawler type:\n- **Adaptive switching between browser and raw HTTP client** - The crawler automatically switches between raw HTTP for static pages and Firefox browser (via Playwright) for dynamic pages, to get the maximum performance wherever possible.\n- **Headless web browser** - Useful for modern websites with\n  anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and\n  automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it\n  requires more computing resources and is slower.\n- **Stealthy web browser** (default) - Another headless web browser, but with anti-blocking measures enabled. Try this if you encounter\n  bot protection while scraping. For best performance, use it with Apify Proxy.\n- **Raw HTTP client** - High-performance crawling mode that uses raw HTTP requests to fetch the pages.\n  It is faster and cheaper, but it might not work on all websites.\n- **Raw HTTP client with JS execution (JSDOM)** [experimental] - A compromise between a browser and raw HTTP crawlers. Good performance and should work on almost all websites, including those with dynamic content.\n  However, it is still experimental and might sometimes crash, so we don\'t recommend it in production settings yet.\n\nYou can also set additional input parameters such as a maximum number of pages, maximum crawling depth,\nmaximum concurrency, proxy configuration, timeout, etc., to control the behavior and performance of the Actor.\n\n### HTML processing\n\nThe goal of the HTML processing step is to ensure each web page has the right content â€” neither less nor more.\n\nIf you\'re using a headless browser **Crawler type**,\nwhenever a web page is loaded,\nthe Actor can wait a certain time or scroll to a certain height\nto ensure all dynamic page content is loaded, using the **Wait for dynamic content** or **Maximum scroll height** input settings, respectively.\nIf **Expand clickable elements** is enabled, the Actor tries to click various DOM\nelements to ensure their content is expanded and visible in the resulting text.\n\nOnce the web page is ready, the Actor\ntransforms its DOM to remove irrelevant content in order to help you ensure you\'re feeding your AI models with relevant data\nto keep them accurate.\n\nFirst, the Actor removes DOM nodes matching the **Remove HTML elements (CSS selector)**. The provided default value attempts\nto remove all common types of modals, navigation, headers, or footers, as well as scripts and inline images\nto reduce the output HTML size.\n\nIf the **Extract HTML elements (CSS selector)** option is specified, the Actor only keeps the contents of the elements targeted by this CSS selector and removes all the other HTML elements from the DOM.\n\nThen, if **Remove cookie warnings** is enabled,\nthe Actor removes cookie warnings using the [I don\'t care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension.\n\nFinally, the Actor transforms the page using the selected **HTML transformer**, whose goal is to only keep the important content of the page and reduce\nits complexity before converting it to text. Basically, to keep just the "meat" of the article or a page.\n\n### File download\n\nIf the `Save files` option is set, the Actor will download "document" files linked from the page. This is limited to PDF, DOC, DOCX, XLS, and XLSX files.\n\nNote that these files are exempt from the URL scoping rules - any file linked from the page will be downloaded, regardless of its URL.\nYou can change this behaviour by using the `Include / Exclude URLs (globs)` setting.\n\nThe hard limit for a single file download time is 1 hour. If the download of a single file takes longer, the Actor will abort the download.\n\nFurthermore, you can specify the minimal file download speed in kilobytes per second. If the download speed stays below this threshold for more than 10 seconds, the download will be aborted.\nThis is configurable via the `Minimal file download speed (KB/s)` setting.\n\n### Output\n\nOnce the web page HTML is processed, the Actor converts it to the desired output format, including plain text, Markdown to preserve rich formatting,\nor save the full HTML or a screenshot of the page, which is useful for debugging.\nThe Actor also saves important metadata about the content, such as author, language, publishing date, etc.\n\nThe results of the actor are stored in the default [Dataset](https://docs.apify.com/platform/storage/dataset) associated\nwith the Actor run, from where you can access it via API and export to formats like JSON, XML, or CSV.\n\n\n## Example\n\nThis example shows how to scrape all pages from the Apify documentation at https://docs.apify.com/:\n\n### Input\n\n![input-screenshot.png](https://apify-uploads-prod.s3.amazonaws.com/d9f0eb8b-ae01-42be-8629-39e659a14ed6_website_content_crawler_input_example.png)\n\n[See full input](https://apify.com/apify/website-content-crawler/input-schema) with description.\n\n### Output\n\nThis is how one crawled page (https://docs.apify.com/academy/web-scraping-for-beginners) looks in a browser:\n\n![page-screenshot.png](https://apify-uploads-prod.s3.amazonaws.com/399ddefd-3877-41e7-86ed-4025cdea46f8_Screenshot2023-03-29at10.56.57.png)\n\nAnd here is how the crawling result looks in JSON format (note that other formats like CSV or Excel are also supported).\nThe main page content can be found in the `text` field, and it only contains the valuable\ncontent, without menus and other noise:\n\n```json\n{\n    "url": "https://docs.apify.com/academy/web-scraping-for-beginners",\n    "crawl": {\n        "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",\n        "loadedTime": "2023-04-05T16:26:51.030Z",\n        "referrerUrl": "https://docs.apify.com/academy",\n        "depth": 0\n    },\n    "metadata": {\n        "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",\n        "title": "Web scraping for beginners | Apify Documentation",\n        "description": "Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.",\n        "author": null,\n        "keywords": null,\n        "languageCode": "en"\n    },\n    "screenshotUrl": null,\n    "text": "Skip to main content\\nOn this page\\nWeb scraping for beginners\\nLearn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.\\nWelcome to Web scraping for beginners, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you\'re looking for a quick start, we recommend trying this tutorial instead.\\nThis course is made by Apify, the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you\'ll be able to run your scrapers on any computer. No Apify account needed.\\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the Apify platform course, where we\'ll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\\nWhy learn scraper development?â€‹\\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technology like React or Next.js will be a piece of cake.\\nCourse Summaryâ€‹\\nWhen we set out to create the Academy, we wanted to build a complete guide to modern web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\\nThis is what you\'ll learn in the Web scraping for beginners course:\\nWeb scraping for beginners\\nBasics of data extraction\\nBasics of crawling\\nBest practices\\nRequirementsâ€‹\\nYou don\'t need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don\'t be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you\'re new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using [] instead of () can make a lot of difference.\\nIf you don\'t already have basic programming knowledge and would like to be well-prepared for this course, we recommend taking a JavaScript course and learning about CSS Selectors.\\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\\nIdeally, you should have at least a moderate understanding of the following concepts:\\nJavaScript + Node.jsâ€‹\\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and async...await), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\\nasync...await (YouTube)\\nJavaScript loops (MDN)\\nModularity in Node.js\\nGeneral web developmentâ€‹\\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because the knowledge of them will be assumed (unless we\'re showing something out of the ordinary).\\nHTML\\nHTTP protocol\\nDevTools\\njQuery or Cheerioâ€‹\\nWe\'ll be using the Cheerio package a lot to parse data from HTML. This package provides a simple API using jQuery syntax to help traverse downloaded HTML within Node.js.\\nNext upâ€‹\\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. So let\'s get to it!\\nIf you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the Basics of crawling section.\\nWhy learn scraper development?\\nCourse Summary\\nRequirements\\nJavaScript + Node.js\\nGeneral web development\\njQuery or Cheerio\\nNext up",\n    "html": null,\n    "markdown": "  Web scraping for beginners | Apify Documentation       \\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)\\n\\nOn this page\\n\\n# Web scraping for beginners\\n\\n**Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.**\\n\\n* * *\\n\\nWelcome to **Web scraping for beginners**, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you\'re looking for a quick start, we recommend trying [this tutorial](https://blog.apify.com/web-scraping-javascript-nodejs/) instead.\\n\\nThis course is made by [Apify](https://apify.com), the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you\'ll be able to run your scrapers on any computer. No Apify account needed.\\n\\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the [Apify platform course](/academy/apify-platform), where we\'ll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\\n\\n## Why learn scraper development?[â€‹](#why-learn \\"Direct link to Why learn scraper development?\\")\\n\\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\\n\\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\\n\\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technology like React or Next.js will be a piece of cake.\\n\\n## Course Summary[â€‹](#summary \\"Direct link to Course Summary\\")\\n\\nWhen we set out to create the Academy, we wanted to build a complete guide to modern web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\\n\\nThis is what you\'ll learn in the **Web scraping for beginners** course:\\n\\n*   [Web scraping for beginners](/academy/web-scraping-for-beginners)\\n    *   [Basics of data extraction](/academy/web-scraping-for-beginners/data-collection)\\n    *   [Basics of crawling](/academy/web-scraping-for-beginners/crawling)\\n    *   [Best practices](/academy/web-scraping-for-beginners/best-practices)\\n\\n## Requirements[â€‹](#requirements \\"Direct link to Requirements\\")\\n\\nYou don\'t need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don\'t be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you\'re new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using `[]` instead of `()` can make a lot of difference.\\n\\n> If you don\'t already have basic programming knowledge and would like to be well-prepared for this course, we recommend taking a [JavaScript course](https://www.codecademy.com/learn/introduction-to-javascript) and learning about [CSS Selectors](https://www.w3schools.com/css/css_selectors.asp).\\n\\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\\n\\nIdeally, you should have at least a moderate understanding of the following concepts:\\n\\n### JavaScript + Node.js[â€‹](#javascript-and-node \\"Direct link to JavaScript + Node.js\\")\\n\\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and `async...await`), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\\n\\n*   [`async...await` (YouTube)](https://www.youtube.com/watch?v=vn3tm0quoqE&ab_channel=Fireship)\\n*   [JavaScript loops (MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Loops_and_iteration)\\n*   [Modularity in Node.js](https://www.section.io/engineering-education/how-to-use-modular-patterns-in-nodejs/)\\n\\n### General web development[â€‹](#general-web-development \\"Direct link to General web development\\")\\n\\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because the knowledge of them will be **assumed** (unless we\'re showing something out of the ordinary).\\n\\n*   [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML)\\n*   [HTTP protocol](https://developer.mozilla.org/en-US/docs/Web/HTTP)\\n*   [DevTools](/academy/web-scraping-for-beginners/data-collection/browser-devtools)\\n\\n### jQuery or Cheerio[â€‹](#jquery-or-cheerio \\"Direct link to jQuery or Cheerio\\")\\n\\nWe\'ll be using the [**Cheerio**](https://www.npmjs.com/package/cheerio) package a lot to parse data from HTML. This package provides a simple API using jQuery syntax to help traverse downloaded HTML within Node.js.\\n\\n## Next up[â€‹](#next \\"Direct link to Next up\\")\\n\\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. So [let\'s get to it!](/academy/web-scraping-for-beginners/introduction)\\n\\n> If you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the [Basics of crawling](/academy/web-scraping-for-beginners/crawling) section.\\n\\n*   [Why learn scraper development?](#why-learn)\\n*   [Course Summary](#summary)\\n*   [Requirements](#requirements)\\n    *   [JavaScript + Node.js](#javascript-and-node)\\n    *   [General web development](#general-web-development)\\n    *   [jQuery or Cheerio](#jquery-or-cheerio)\\n*   [Next up](#next)"\n}\n```\n\n## Integration with the AI ecosystem\n\nThanks to the native [Apify platform integrations](https://docs.apify.com/platform/integrations),\nWebsite Content Crawler can seamlessly connect with various third-party\nsystems and tools.\n\n### Exporting GPT knowledge files\n\nApify allows you to seamlessly export the results of Website Content Crawler runs to your custom GPTs.\n\nTo do this, go to the Output tab of the Actor run and click the "Export results" button. From here, pick `JSON` and click "Export". You can then upload the JSON file to your custom GPTs.\n\nFor a step-by-step guide, see [How to add a knowledge base to your GPTs](https://blog.apify.com/custom-gpts-knowledge/#how-to-add-knowledge-to-gpts-step-by-step-guide).\n\n### LangChain integration\n\n[LangChain](https://github.com/hwchase17/langchain) is the most popular framework for\ndeveloping applications powered by language models.\nIt provides an [integration for Apify](https://python.langchain.com/en/latest/modules/agents/tools/examples/apify.html),\nso you can feed Actor results directly to LangChainâ€™s vector databases,\nenabling you to easily create ChatGPT-like query interfaces to\nwebsites with documentation, knowledge base, blog, etc.\n\n\n#### Python example\n\nFirst, install LangChain with OpenAI LLM and Apify API client for Python:\n\n```bash\npip install apify-client langchain langchain_community langchain_openai openai tiktoken\n```\n\nAnd then create a ChatGPT-powered answering machine:\n\n```python\nimport os\n\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_community.utilities import ApifyWrapper\nfrom langchain_core.document_loaders.base import Document\nfrom langchain_openai import OpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# Set up your Apify API token and OpenAI API key\nos.environ["OPENAI_API_KEY"] = "Your OpenAI API key"\nos.environ["APIFY_API_TOKEN"] = "Your Apify API token"\n\napify = ApifyWrapper()\n\n# Run the Website Content Crawler on a website, wait for it to finish, and save its results into a LangChain document loader:\nloader = apify.call_actor(\n    actor_id="apify/website-content-crawler",\n    run_input={"startUrls": [{"url": "https://docs.apify.com/"}], "maxCrawlPages": 10},\n    dataset_mapping_function=lambda item: Document(\n        page_content=item["text"] or "", metadata={"source": item["url"]}\n    ),\n)\n# Initialize the vector database with the text documents:\nindex = VectorstoreIndexCreator(embedding=OpenAIEmbeddings()).from_loaders([loader])\n\n# Finally, query the vector database:\nquery = "What is Apify?"\nresult = index.query_with_sources(query, llm=OpenAI())\n\nprint("answer:", result["answer"])\nprint("source:", result["sources"])\n```\n\nThe query produces an answer like this:\n\n> _Apify is a platform for developing, running, and sharing serverless cloud programs. It enables users to create web scraping and automation tools and publish them on the Apify platform._\n>\n> https://docs.apify.com/platform/actors, https://docs.apify.com/platform/actors/running/actors-in-store, https://docs.apify.com/platform/security, https://docs.apify.com/platform/actors/examples\n\nFor details and Jupyter notebook, see [Apify integration for LangChain](https://python.langchain.com/docs/integrations/tools/apify/).\n\n#### Node.js example\n\nSee [detailed example](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/web_loaders/apify_dataset) in LangChain for JavaScript.\n\n### LlamaIndex integration\n\n[LlamaIndex](https://docs.llamaindex.ai/) is a Python library that provides a central interface to connect LLMs with external data.\nThe [Apify integration](https://llamahub.ai/l/readers/llama-index-readers-apify?from=) makes it easy to feed LlamaIndex applications with data crawled from the web.\n\nInstall all required packages:\n\n```bash\npip install apify-client llama-index-core llama-index-readers-apify\n```\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.readers.apify import ApifyActor\n\nreader = ApifyActor("<My Apify API token>")\n\ndocuments = reader.load_data(\n    actor_id="apify/website-content-crawler",\n    run_input={\n        "startUrls": [{"url": "https://docs.llamaindex.ai/en/latest/"}]\n    },\n    dataset_mapping_function=lambda item: Document(\n        text=item.get("text"),\n        metadata={\n            "url": item.get("url"),\n        },\n    ),\n)\n```\n\n### Vector database integrations (Pinecone, Qdrant)\n\nWebsite Content Crawler can be easily integrated with vector databases to store the crawled data for semantic search.\nUsing Apify\'s [Pinecone](https://apify.com/apify/pinecone-integration) or [Qdrant](https://apify.com/apify/qdrant-integration) integration Actors, you can upload the results of Website Content Crawler directly into a vector database.\nThe integrations support incremental updates, updating only the data that has changed since the last crawl.\nThis helps to reduce costly embedding computation and storage operations, making it suitable for regular updates of large websites.\nJust set up the Pinecone integration Actor with Website Content Crawler using this [step-by-step guide](https://docs.apify.com/platform/integrations/pinecone).\n\n### GPT integration\n\nYou can use Website Content Crawler to add knowledge to your GPTs. Crawl a website and upload the scraped dataset to your custom GPT.\nThe video tutorial below demonstrates how it works.\n\n[How to add a knowledge base to your GPTs](https://www.youtube.com/watch?v=z552gt-3Ce0)\n\nYou can also use the Website Content Crawler together with the OpenAI Assistant to update its knowledge base with web content using the [OpenAI VectorStore Integration](https://apify.com/jiri.spilka/openai-vector-store-integration).\n\n\n## How much does it cost?\n\nWebsite Content Crawler is free to useâ€”you only pay for the Apify platform usage consumed by the Actor.\nThe exact price depends on the crawler type and settings, website complexity, network speed,\nand random circumstances.\n\nThe main cost driver of Website Content Crawler is the compute power, which is measured in the Actor compute units (CU):\n1 CU corresponds to an actor with 1 GB of\nmemory running for 1 hour. With the baseline price of $0.25/CU, from our tests, the actor usage costs **approximately**:\n\n- $0.5 - $5 per 1,000 web pages with a headless browser, depending on the website\n- $0.2 per 1,000 web pages with raw HTTP crawler\n\nNote that Apify\'s free plan gives you $5 free credits every month and access to [Apify Proxy](https://apify.com/proxy),\nwhich is sufficient for testing and low-volume use cases.\n\n## Troubleshooting\n\n- The Actor works best for crawling sites with multiple URLs. For **extracting text or Markdown from a single URL**,\n  you might prefer to use [RAG Web Browser](https://apify.com/apify/rag-web-browser) in the Standby mode,\n  which is much faster and more efficient.\n- If the **extracted text doesnâ€™t contain the expected page content**, try to select another _Crawler type_.\n  Generally, a headless browser will extract more text as it loads dynamic page content\n  and is less likely to be blocked.\n- If the **extracted text has more than expected page content** (e.g. navigation or footer),\n  try to select another _HTML transformer_, or use the _Remove HTML elements_ setting\n  to skip unwanted parts of the page.\n- If the **crawler is too slow**, try increasing the Actor memory and/or the _Initial concurrency_ setting.\n  Note that if you set the concurrency too high, the Actor will run out of memory and crash,\n  or potentially overload the target site.\n- If the target website is blocking the crawler, make sure to use the **Stealthy web browser (Firefox+Playwright)**\n  crawler type and use residential proxies\n- The crawler **automatically restarts on crash**, and continues where it left off.\n  But if it crashes more than 3 times per minute, the system fails the Actor run.\n\n## Help & support\n\nWebsite Content Crawler is under active development.\nIf you have any feedback or feature ideas, please [submit an issue](https://console.apify.com/actors/aYG0l9s7dbB7j3gbS/issues).\n\n## Is it legal?\n\nWeb scraping is generally legal if you scrape publicly available non-personal data. What you do with the data is another question.\nDocumentation, help articles, or blogs are typically protected by copyright, so you can\'t republish the content without the owner\'s permission.\n\nLearn more about the legality of web scraping in this\n[blog post](https://blog.apify.com/is-web-scraping-legal/). If you\'re not sure, please seek professional legal advice.\n',
			changelog:
				'# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n\n## 0.3.67 (2025-06-19)\n\n### ðŸš€ Features\n\n- Lower default `maxRequestRetries` limit to `3` (#419)\n- Add `ignoreHttpsErrors` input option (#422)\n- Add a flag to disable image and video loading in headless browser (#423)\n\n### ðŸ› Bug Fixes\n\n- `keepElementsCssSelector` doesn&#x27;t affect link enqueuing (#413)\n- Clean up after `SitemapRequestList` to avoid excessive timeouts (#415)\n\n\n\n## 0.3.66 (2025-05-26)\n\n### ðŸš€ Features\n\n- Use code from the Ghostery adblocker to remove cookie modals (#403)\n\n### ðŸ› Bug Fixes\n\n- Only use titles from the document head for the metadata section (#401)\n- Store downloaded HTML explicitly as UTF-8 (#410)\n\n### âš¡ Performance\n\n- Add more &quot;unparseable&quot; file types (`jpg`, `png`, `gif`...) (#402)\n\n\n\n## 0.3.65 (2025-04-17)\n\n### ðŸš€ Features\n\n- Allow toggling the crawlee respectRobotsTxtFile via input (#392)\n- Sticky containers (#390)\n\n### ðŸ› Bug Fixes\n\n- Fix SitemapRequestList persistence (#385)\n\n\n\n## 0.3.64 (2025-03-21)\n\n### ðŸš€ Features\n\n- Handle more types of cookie modals (#378)\n- Store original file name and size for downloads (#377)\n\n\n\n## 0.3.63 (2025-03-17)\n\n### ðŸš€ Features\n\n- Use RequestQueueV1 (#376)\n\n\n\n##  (2025-03-14)\n\n### ðŸ› Bug Fixes\n\n- Automatically close cookie modals by usercentrics.eu (#373)\n\n\n\n## 0.3.61 (2025-03-04)\n\n### ðŸ› Bug Fixes\n\n- Generate unique KVS keys for `saveHtmlAsFile` results (#369)\n- Correctly validate `waitForSelector` input option (#370)\n\n\n\n## 0.3.60 (2025-02-21)\n\n### ðŸš€ Features\n\n- Add support for `softWaitForSelector` (#365)\n\n### ðŸ› Bug Fixes\n\n- More stable crawler stopping (#366)\n\n\n\n## 0.3.59 (2025-01-22)\n\n### ðŸ› Bug Fixes\n\n- Work around missing `content-type` header in `FileDownload` (#360)\n- Extract only top-level `title` (skip iframes) (#362)\n\n\n\n## 0.3.58 (2025-01-10)\n\n### ðŸš€ Features\n\n- Push `latest` to Apify with GH actions, add auto changelog bumps (#347)\n- More lenient behaviour on missing `content-type` response header (#353)\n- Improve antiblocking performance (#354)\n\n\n## 0.3.57 (2024-12-13)\n\nThis release only contains documentation changes in readme and input schema.\n\n## 0.3.56 (2024-11-25)\n- Input:\n    - Empty `includeUrlGlobs` are now filtered out with a warning log message. To enforce the old behavior (i.e. matching everything), use `**` instead.\n- Behaviour:\n    - The Actor now automatically drops the request queue associated with the file download.\n    - Only the first `<title>` element on the page is prepended to the exported content.\n    - The crawler now uses the correct scope with all `startUrls` being sitemaps.\n    - Sitemaps are now processed in a separate thread. The 30 seconds per sitemap request limit is now strongly enforced.\n\n## 0.3.55 (2024-11-11)\n- Behaviour:\n    - The sitemap timeout warning is only logged the first time.\n\n## 0.3.54 (2024-11-07)\n- Input:\n    - The default `removeElementsCssSelector` now removes `img` elements with `data:` URLs to prevent cluttering the text output.\n\n- Behaviour:\n    - `expandIframes` now skips broken `iframe` elements instead of failing the whole request.\n    - Actor now parses formatted (indented or newline-separated) sitemaps correctly.\n    - The sitemap discovery process is now parallelized. Logging is improved to show the progress of the sitemap discovery.\n    - Sitemap processing now has stricter per-URL time limits to prevent indefinite hangs.\n\n## 0.3.53 (2024-10-22)\n- Input:\n    - New `keepElementsCssSelector` accepts a CSS selector targetting elements to extract from the page to the output.\n- Behaviour:\n    - Actor optimizes RQ writes by following the `maxCrawlPages` limits better.\n\n## 0.3.52 (2024-10-10)\n- Behavior:\n    - Handle sitemap-based requests correctly. Solves the indefinite `RequestQueue` hanging read issue.\n\n## 0.3.51 (2024-10-10)\n- Behavior:\n    - Revert internal library update to mitigate the indefinite `RequestQueue` hanging read issue.\n\n## 0.3.50 (2024-10-07)\n- Behavior:\n    - Actor terminates sitemap loading prematurely in case of a timeout.\n    - Sitemap loading now respects `maxRequestRetries` option.\n\n## 0.3.49 (2024-09-23)\n- Behavior:\n    - Use the correct proxy settings when loading sitemap files.\n    - Mitigate sitemap persistence issues with premature stopping on `maxRequests` (`ERR_STREAM_PUSH_AFTER_EOF`).\n\n## 0.3.48 (2024-09-10)\n- Input:\n    - `useSitemaps` option is now pre-filled to `true` to automatically enable it for new users and in API examples.\n\n## 0.3.47 (2024-09-04)\n- Behaviour:\n    - Use crawlee 3.11.3 which should help with the crawler being stuck because of some stale requests in the queue.\n\n## 0.3.46 (2024-08-30)\n- Behaviour:\n    - Process markdown in a separate worker thread so it won\'t block the main process on too large pages.\n    - Sitemap loading with `useSitemaps` doesn\'t block indefinitely on large sitemaps anymore.\n\n## 0.3.45 (2024-08-20)\n- Input:\n    - New `keepFragmentUrls` (URL `#fragments` identify unique pages) input option to consider fragment URLs as separate pages.\n- Behaviour:\n    - Ensure canonical URL is only taken from the main page and not embedded pages from `<iframe>`.\n    - Too large dataset items used to fail, now we retry with a trimmed payload (`html`, `text` and `markdown` fields are trimmed to the first three million characters each).\n\n## 0.3.44 (2024-07-30)\n- Behaviour:\n    - `waitForSelector` option allows users to specify a CSS selector to wait for before extracting the page content. This is useful for pages that load content dynamically and break the automatic waiting mechanism.\n\n## 0.3.43 (2024-07-24)\n- Behaviour:\n    - Change the shadow DOM expansion logic to handle edge cases better.\n\n## 0.3.42 (2024-07-12)\n- Behaviour:\n    - Fix edge cases with the improved `startUrls` sanitization.\n\n## 0.3.41 (2024-07-11)\n- Behaviour:\n    - Better input URLs sanitization to prevent issues with the `startUrls` input.\n\n## 0.3.40 (2024-07-10)\n- Input:\n    - New `expandIframes` (Expand iframe elements) option for extracting content from on-page `iframe` elements. Available only in `playwright:firefox`.\n\n## 0.3.39 (2024-06-28)\n- Behaviour:\n    - Mitigating excessive Request Queue writes in some cases.\n\n## 0.3.38 (2024-06-25)\n- Behaviour:\n    - The `saveScreenshots` option now correctly prints warnings with crawler types that don\'t support screenshots.\n    - The screenshot KVS key now contains the website hostname and a hash of the original URL to avoid collisions.\n\n## 0.3.37 (2024-06-17)\n- Behaviour:\n    - The Actor now respects the advanced request configuration passed through the `Start URLs` input.\n\n## 0.3.36 (2024-06-10)\n- Input:\n    - New `saveHtmlAsFile` option is available which enables storing the HTML into a key-value store, replacing their values in the dataset with links to make the dataset value smaller, since there is a hard limit for its size.\n    - Deprecated `saveHtml` in favor of `saveHtmlAsFile`.\n- Output:\n    - The new `saveHtmlAsFile` option saves the URL under a new `htmlUrl` key in the dataset.\n- Behaviour:\n    - HTML processors don\'t block the main thread and can safely time out.\n    - Better fallback logic for the HTML processing pipeline.\n    - When pushing to dataset, we now detect too large payload and skip retrying (while suggesting to use the new `saveHtmlAsFile` option to get around this problem).\n\n## 0.3.35 (2024-05-23)\n- Behaviour:\n    - `RequestQueue` race condition fixed.\n- Output:\n    - The Readable text extractor now correctly handles the article titles.\n\n## 0.3.34 (2024-05-17)\n- Behaviour:\n    - If any of the Start URLs lead to a sitemap file, it is processed and the links are enqueued.\n    - Performance / QoL improvements (see [Crawlee 3.10.0 changelog](https://github.com/apify/crawlee/releases/tag/v3.10.0) for more details)\n\n## 0.3.33 (2024-04-22)\n- Input:\n    - `AdaptiveCrawler` is the new default (prefill) crawler type.\n- Behaviour:\n    - The use of Chrome + Chromium browsers was deprecated. The Actor now uses only Firefox internally for the browser-based crawlers.\n    - Reimplementation of the file download feature for better stability and performance.\n    - On smaller websites, the `AdaptiveCrawler` skips the adaptive scanning to speed up the crawl.\n\n## 0.3.32 (2024-03-28)\n- Output:\n    - The Actor now stores `metadata.headers` with the HTTP response headers of the crawled page.\n\n## 0.3.31 (2024-03-14)\n- Input:\n    - Invalid `startUrls` are now filtered out and don\'t cause the actor to fail.\n\n## 0.3.30 (2024-02-24)\n- Behavior:\n    - File download now respects `excludeGlobs` and `includeGlobs` input options, stores filenames, and understands `Content-Disposition` HTTP headers (i.e. "forced download").\n- Output:\n    - Better `og:` metatags coverage (`article:`, `movie:` etc.).\n    - Stores JSON-LD metatags in the `metadata` field.\n\n## 0.3.29 (2024-02-05)\n- Input:\n    - `useSitemaps` toggle for sitemap discovery - leads to more consistent results, scrapes also unreachable webpages.\n    - Do not fail on empty globs in input (ignore them instead).\n    - Experimental `playwright:adaptive` crawling mode\n- Output:\n    - Add `metadata.openGraph` output field for contents of `og:*` metatags.\n\n## 0.3.27 (2024-01-25)\n- Input:\n    - `maxRequestRetries` input option for limiting the number of request retries on network, server or parsing errors.\n- Behavior:\n    - Allow large lists of start URLs with deep crawling (`maxCrawlDepth > 0`), as the memory overflow issue from `0.3.18` is now fixed.\n\n## 0.3.26 (2024-01-02)\n- Output:\n    - The Actor now stores `metadata.mimeType` for downloaded files (only applicable when `saveFiles` is enabled).\n\n## 0.3.25 (2023-12-21)\n- Input:\n    - Add `maxSessionRotations` input option for limiting the number of session rotations when recognized as a bot.\n- Behavior:\n    - Fail on `401`, `403` and `429` HTTP status codes.\n\n## 0.3.24 (2023-12-07)\n- Behavior:\n    - Fix a bug within the `expandClickableElements` utility function.\n\n## 0.3.23 (2023-12-06)\n- Behavior:\n    - Respect empty `<body>` tag when extracting text from HTML.\n    - Fix a bug with the `simplifiedBody === null` exception.\n\n## 0.3.22 (2023-12-04)\n- Input:\n    - `ignoreCanonicalUrl` toggle to deduplicate pages based on their actual URLs (useful when two different pages share the same canonical URL).\n- Behavior:\n    - Improve the large content detection - this fixes a regression from `0.3.21`.\n\n## 0.3.21 (2023-11-29)\n- Output:\n    - The `debug` mode now stores the results of all the extractors (+ raw HTML) as Key-Value Store objects.\n    - New extractor "Readable text with fallback" checks the results of the "Readable text" extractor and checks the content integrity on the fly.\n- Behavior:\n    - Skip text-cleaning and markdown processing step on large responses to avoid indefinite hangs.\n\n## 0.3.20 (2023-11-08)\n- Output:\n    - The `debug` mode now stores the raw page HTML (without the page preprocessing) under the `rawHtml` key.\n\n## 0.3.19 (2023-10-18)\n- Input:\n    - Add a default for `proxyConfiguration` option (which is now required since 0.3.18). This fixes the actor usage via API, falling back to the default proxy settings if they are not explicitly provided.\n\n## 0.3.18 (2023-10-17)\n- Input:\n    - Adds `includeUrlGlobs` option to allow explicit control over enqueuing logic (overrides the default scoping logic).\n    - Adds `requestTimeoutSecs` option to allow overriding the default request processing timeout.\n- Behavior:\n    - Disallow using large list of start URLs (more than 100) with deep crawling (`maxCrawlDepth > 0`) as it can lead to memory overflow.\n\n## 0.3.17 (2023-10-05)\n- Input:\n    - Adds `debugLog` option to enable debug logging.\n\n## 0.3.16 (2023-09-06)\n- Behavior:\n    - Raw HTTP Client (Cheerio) now works correctly with proxies again.\n\n## 0.3.15 (2023-08-30)\n- Input:\n    - `startUrls` is now a required input field.\n    - Input tooltips now provide more detailed description of crawler types and other input options.\n\n## 0.3.14 (2023-07-19)\n- Behavior:\n    - When using the Cheerio based crawlers, the actor processes links from removed elements correctly now.\n    - Crawlers now follow links in `<link>` tags (`rel=next, prev, help, search`).\n    - Relative canonical URLs are now getting correctly resolved during the deduplication phase.\n    - Actor now automatically recognizes blocked websites and retries the crawl with a new proxy / fingerprint combination.\n\n## 0.3.13 (2023-06-30)\n- Input:\n    - The Actor now uses new defaults for input settings for better user experience:\n        - default `crawlerType` is now `playwright:firefox`\n        - `saveMarkdown` is `true`\n        - `removeCookieWarnings` is `true`\n\n## 0.3.12 (2023-06-14)\n- Input:\n    - add `excludeUrlGlobs` for skipping certain URLs when enqueueing\n    - add `maxScrollHeightPixels` for scrolling down the page (useful for dynamically loaded pages)\n- Behavior:\n    - by default, the crawler scrolls down on every page to trigger dynamic loading (disable by setting `maxScrollHeightPixels` to 0)\n    - the crawler now handles HTML processing errors gracefully\n    - the actor now stays alive and restarts the crawl on certain known errors (Playwright Assertion Error).\n- Output:\n    - `crawl.httpStatusCode` now contains HTTP response status code.\n\n## 0.3.10 (2023-06-05)\n- Input:\n    - move `processedHtml` under `debug` object\n    - add `removeCookieWarnings` option to automatically remove cookie consent modals (via [I don\'t care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension)\n- Behavior:\n    - consider redirected start URL for prefix matching\n    - make URL deduplication case-insensitive\n    - wait at least 3s in playwright to let dynamic content to load\n    - retry the start URLs 10 times and regular URLs 5 times (to get around issues with retries on burnt proxies)\n    - ignore links not starting with `http`\n    - skip parsing non-html files\n    - support `startUrls` as text-file\n\n## 0.3.9 (2023-05-18)\n- Input:\n    - Updated README and input hints.\n\n## 0.3.8 (2023-05-17)\n- Input:\n    - `initialCookies` option for passing cookies to the crawler. Provide the cookies as a JSON array of objects with `"name"` and `"value"` keys. Example: `[{ "name": "token", "value": "123456" }]`.\n- Behavior:\n    - `textExtractor` option is now removed in favour of `htmlTransformer`\n    - `unfluff` extractor has been completely removed\n    - HTML is always simplified by removing some elements from it, those are configurable via `removeElementsCssSelector` option, which now defaults to a larger set of elements, including `<nav>`, `<footer>`, `<svg>`, and elements with `role` attribute set to one of `alert`, `banner`, `dialog`, `alertdialog`.\n    - New `htmlTransformer` option has been introduced which allows to configure how the simplified HTML is further processed. The output of this is still an HTML, which can be later used to generate markdown or plain text from it.\n    - The crawler will now try to expand collapsible sections automatically. This works by clicking on elements with `aria-expanded="false"` attribute. You can configure this selector via `clickElementsCssSelector` option.\n    - When using Playwright based crawlers, the dynamic content is awaited for based on network activity, rather than webpage changes. This should improve the reliability of the crawler.\n    - Firefox `SEC_ERROR_UNKNOWN_ISSUER` has been solved by preloading the recognized intermediate TLS certificates into the Docker image.\n    - Crawled URLs are retried in case their processing timeouts.\n\n## 0.3.7 (2023-05-10)\n- Behavior:\n    - URLs with redirects are now enqueued based on the original (unredirected) URL. This should prevent the actor from skipping relevant pages which are hidden behind redirects.\n    - The actor now considers all start URLs when enqueueing new links. This way, the user can specify multiple start URLs as a workaround for the actor skipping some relevant pages on the website.\n    - Error with not enqueueing URLs with certain query parameters is now fixed.\n- Output:\n    - The `.url` field now contains the main resource URL without the fragment (`#`) part.\n\n## 0.3.6 (2023-05-04)\n- Input:\n    - Made the `initialConcurrency` option visible in the input editor.\n    - Added `aggressivePruning` option. With this option set to `true`, the crawler will try to deduplicate the scraped content. This can be useful when the crawler is scraping a website with a lot of duplicate content (header menus, footers, etc.)\n- Behavior:\n    - The actor now stays alive and restarts the crawl on certain known errors (Playwright Assertion Error).\n\n## 0.3.4 (2023-05-04)\n- Input:\n   - Added a new hidden option `initialConcurrency`. This option sets the initial number of web browsers or HTTP clients running in parallel during the actor run. Increasing this number can speed up the crawling process. Bear in mind this option is hidden and can be changed only by editing the actor input using the JSON editor.\n\n## 0.3.3 (2023-04-28)\n- Input:\n   - Added a new option `maxResults` to limit the total number of results. If used with `maxCrawlPages`, the crawler will stop when either of the limits is reached.\n\n## 0.3.1 (2023-04-24)\n- Input:\n   - Added an option to download linked document files from the page - `saveFiles`. This is useful for downloading pdf, docx, xslx... files from the crawled pages. The files are saved to the default key-value store of the run and the links to the files are added to the dataset.\n   - Added a new crawler - Stealthy web browser - that uses a Firefox browser with a stealthy profile. It is useful for crawling websites that block scraping.\n\n## 0.0.13 (2023-04-18)\n- Input:\n    - Added new `textExtractor` option `readableText`. It is generally very accurate and has a good ratio of coverage to noise. It extracts only the main article body (similar to `unfluff`) but can work for more complex pages.\n    - Added `readableTextCharThreshold` option. This only applies to `readableText` extractor. It allows fine-tuning which part of the text should be focused on. That only matters for very complex pages where it is not obvious what should be extracted.\n- Output:\n    - Added simplified output view `Overview` that has only `url` and `text` for quick output check\n- Behavior:\n    - Domains starting with `www.` are now considered equal to ones without it. This means that the start URL `https://apify.com` can enqueue  `https://www.apify.com` and vice versa.\n\n## 0.0.10 (2023-04-05)\n- Input:\n    - Added new `crawlerType` option `jsdom` for processing with JSDOM. It allows client-side script processing, trying to mimic the browser behavior in Node.js but with much better performance. This is still experimental and may crash on some particular pages.\n    - Added `dynamicContentWaitSecs` option (defaults to 10s), which is the maximum waiting time for dynamic waiting.\n- Output (BREAKING CHANGE):\n    - Renamed `crawl.date` to `crawl.loadedTime`\n    - Moved `crawl.screenshotUrl` to top-level object\n    - The `markdown` field was made visible\n    - Renamed `metadata.language` to `metadata.languageCode`\n    - Removed `metadata.createdAt` (for now)\n    - Added `metadata.keywords`\n- Behavior:\n    - Added waiting for dynamically rendered content (supported in Headless browser and JSDOM crawlers). The crawler checks every half a second for content changes. When there are no changes for 2 seconds, the crawler proceeds to extraction.\n\n## 0.0.7 (2023-03-30)\n- Input:\n    - BREAKING CHANGE: Added `textExtractor` input option to choose how strictly to parse the content. Swapped the previous `unfluff` for `CrawleeHtmlToText` as default which in general will extract more text. We chose to output more text rather than less by default.\n    - Added `removeElementsCssSelector` which allows passing extra CSS selectors to further strip down the HTML before it is converted to text. This can help fine-tuning. By default, the actor removes the page navigation bar, header, and footer.\n- Output:\n    - Added markdown to output if `saveMarkdown` option is chosen\n    - All extractor outputs + HTML as a link can be obtained if `debugMode` is set.\n    - Added `pageType` to the output (only as `debug` for now), it will be fine-tuned in the future.\n- Behavior:\n    - Added deduplication by `canonicalUrl`. E.g. if more different URLs point to the same canonical URL, they are skipped\n    - Skip pages that redirect outside the original start URLs domain.\n    - Only run a single text extractor unless in debug mode. This improves performance.',
			actorDefinition: {
				actorSpecification: 1,
				name: 'website-content-crawler',
				title: 'Website Content Crawler',
				description: '...',
				version: '0.0',
				meta: {
					templateId: 'ts-crawlee-cheerio',
				},
				input: {
					title: 'Input schema for Website Content Crawler',
					description:
						'Enter **Start URLs** of websites to crawl, ensure you are using the right **Crawler type**, check the other optional settings, and click **Start** to run the crawler.',
					type: 'object',
					schemaVersion: 1,
					properties: {
						startUrls: {
							title: 'Start URLs',
							type: 'array',
							description:
								'One or more URLs of pages where the crawler will start.\n\nBy default, the Actor will also crawl sub-pages of these URLs. For example, for start URL `https://example.com/blog`, it will crawl also `https://example.com/blog/post` or `https://example.com/blog/article`. The **Include URLs (globs)** option overrides this automation behavior.',
							editor: 'requestListSources',
							prefill: [
								{
									url: 'https://docs.apify.com/academy/web-scraping-for-beginners',
								},
							],
						},
						useSitemaps: {
							title: 'Load URLs from Sitemaps',
							type: 'boolean',
							description:
								'If enabled, the crawler will look for [Sitemaps](https://en.wikipedia.org/wiki/Sitemaps) at the domains of the provided *Start URLs* and enqueue matching URLs similarly as the links found on crawled pages. You can also reference a `sitemap.xml` file directly by adding it as another Start URL (e.g. `https://www.example.com/sitemap.xml`)\n\nThis feature makes the crawling more robust on websites that support Sitemaps, as it includes pages that might be not reachable from Start URLs. However, **loading and processing Sitemaps can take a lot of time, especially for large sites**. Note that if a page is found via Sitemaps, it will have `depth` of `1`.',
							default: false,
							prefill: false,
						},
						respectRobotsTxtFile: {
							title: 'Respect the robots.txt file',
							type: 'boolean',
							description:
								'If enabled, the crawler will consult the robots.txt file for the target website before crawling each page. At the moment, the crawler does not use any specific user agent identifier. The crawl-delay directive is also not supported yet.',
							default: false,
							prefill: true,
						},
						crawlerType: {
							title: 'Crawler type',
							type: 'string',
							enum: [
								'playwright:adaptive',
								'playwright:firefox',
								'cheerio',
								'jsdom',
								'playwright:chrome',
							],
							enumTitles: [
								'Adaptive switching between browser and raw HTTP - Fast and renders JavaScript content if present. This is the recommended option.',
								'Headless browser (Firefox+Playwright) - Reliable, renders JavaScript content, best in avoiding blocking, but might be slow.',
								"Raw HTTP client (Cheerio) - Fastest, but doesn't render JavaScript content.",
								'Raw HTTP client with JavaScript (JSDOM) - Experimental, use at your own risk.',
								'[DEPRECATED] Headless browser (Chrome+Playwright) - The crawler will use Firefox+Playwright instead.',
							],
							description:
								'Select the crawling engine:\n- **Headless web browser** - Useful for modern websites with anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it requires more computing resources and is slower. It is recommended to use at least 8 GB of RAM.\n- **Stealthy web browser** (default) - Another headless web browser with anti-blocking measures enabled. Try this if you encounter bot protection while scraping. For best performance, use with Apify Proxy residential IPs. \n- **Adaptive switching between Chrome and raw HTTP client** - The crawler automatically switches between raw HTTP for static pages and Chrome browser (via Playwright) for dynamic pages, to get the maximum performance wherever possible. \n- **Raw HTTP client** - High-performance crawling mode that uses raw HTTP requests to fetch the pages. It is faster and cheaper, but it might not work on all websites.\n\nBeware that with the raw HTTP client or adaptive crawling mode, some features are not available, e.g. wait for dynamic content, maximum scroll height, or remove cookie warnings.',
							default: 'playwright:firefox',
							prefill: 'playwright:adaptive',
						},
						includeUrlGlobs: {
							sectionCaption: 'Crawler settings',
							title: 'Include URLs (globs)',
							type: 'array',
							description:
								'Glob patterns matching URLs of pages that will be included in crawling. \n\nSetting this option will disable the default Start URLs based scoping and will allow you to customize the crawling scope yourself. Note that this affects only links found on pages, but not **Start URLs** - if you want to crawl a page, make sure to specify its URL in the **Start URLs** field. \n\nFor example `https://{store,docs}.example.com/**` lets the crawler to access all URLs starting with `https://store.example.com/` or `https://docs.example.com/`, and `https://example.com/**/*\\?*foo=*` allows the crawler to access all URLs that contain `foo` query parameter with any value.\n\nLearn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fscrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fscrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fscrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this).',
							editor: 'globs',
							prefill: [],
							default: [],
						},
						excludeUrlGlobs: {
							title: 'Exclude URLs (globs)',
							type: 'array',
							description:
								'Glob patterns matching URLs of pages that will be excluded from crawling. Note that this affects only links found on pages, but not **Start URLs**, which are always crawled. \n\nFor example `https://{store,docs}.example.com/**` excludes all URLs starting with `https://store.example.com/` or `https://docs.example.com/`, and `https://example.com/**/*\\?*foo=*` excludes all URLs that contain `foo` query parameter with any value.\n\nLearn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fscrape_this).',
							editor: 'globs',
							prefill: [],
							default: [],
						},
						keepUrlFragments: {
							title: 'URL #fragments identify unique pages',
							type: 'boolean',
							description:
								"Indicates that URL fragments (e.g. <code>http://example.com<b>#fragment</b></code>) should be included when checking whether a URL has already been visited or not. Typically, URL fragments are used for page navigation only and therefore they should be ignored, as they don't identify separate pages. However, some single-page websites use URL fragments to display different pages; in such a case, this option should be enabled.",
							default: false,
						},
						ignoreCanonicalUrl: {
							title: 'Ignore canonical URLs',
							type: 'boolean',
							description:
								'If enabled, the Actor will ignore the canonical URL reported by the page, and use the actual URL instead. You can use this feature for websites that report invalid canonical URLs, which causes the Actor to skip those pages in results.',
							default: false,
						},
						ignoreHttpsErrors: {
							title: 'Ignore HTTPS errors',
							type: 'boolean',
							description:
								'If enabled, the scraper will ignore HTTPS certificate errors. Use at your own risk.',
							default: false,
						},
						maxCrawlDepth: {
							title: 'Max crawling depth',
							type: 'integer',
							description:
								'The maximum number of links starting from the start URL that the crawler will recursively follow. The start URLs have depth `0`, the pages linked directly from the start URLs have depth `1`, and so on.\n\nThis setting is useful to prevent accidental crawler runaway. By setting it to `0`, the Actor will only crawl the Start URLs.',
							minimum: 0,
							default: 20,
						},
						maxCrawlPages: {
							title: 'Max pages',
							type: 'integer',
							description:
								'The maximum number pages to crawl. It includes the start URLs, pagination pages, pages with no content, etc. The crawler will automatically finish after reaching this number. This setting is useful to prevent accidental crawler runaway.',
							minimum: 0,
							default: 9999999,
						},
						initialConcurrency: {
							title: 'Initial concurrency',
							type: 'integer',
							description:
								'The initial number of web browsers or HTTP clients running in parallel. The system scales the concurrency up and down based on the current CPU and memory load. If the value is set to 0 (default), the Actor uses the default setting for the specific crawler type.\n\nNote that if you set this value too high, the Actor will run out of memory and crash. If too low, it will be slow at start before it scales the concurrency up.',
							minimum: 0,
							maximum: 999,
							default: 0,
						},
						maxConcurrency: {
							title: 'Max concurrency',
							type: 'integer',
							description:
								'The maximum number of web browsers or HTTP clients running in parallel. This setting is useful to avoid overloading the target websites and to avoid getting blocked.',
							minimum: 1,
							maximum: 999,
							default: 200,
						},
						initialCookies: {
							title: 'Initial cookies',
							type: 'array',
							description:
								'Cookies that will be pre-set to all pages the scraper opens. This is useful for pages that require login. The value is expected to be a JSON array of objects with `name` and `value` properties. For example: `[{"name": "cookieName", "value": "cookieValue"}]`.\n\nYou can use the [EditThisCookie](https://chrome.google.com/webstore/detail/editthiscookie/fngmhnnpilhplaeedifhccceomclgfbg) browser extension to copy browser cookies in this format, and paste it here.',
							default: [],
							prefill: [],
							editor: 'json',
						},
						proxyConfiguration: {
							title: 'Proxy configuration',
							type: 'object',
							description:
								'Enables loading the websites from IP addresses in specific geographies and to circumvent blocking.',
							default: {
								useApifyProxy: true,
							},
							prefill: {
								useApifyProxy: true,
							},
							editor: 'proxy',
						},
						maxSessionRotations: {
							title: 'Maximum number of session rotations',
							type: 'integer',
							description:
								'The maximum number of times the crawler will rotate the session (IP address + browser configuration) on anti-scraping measures like CAPTCHAs. If the crawler rotates the session more than this number and the page is still blocked, it will finish with an error.',
							minimum: 0,
							maximum: 20,
							default: 10,
						},
						maxRequestRetries: {
							title: 'Maximum number of retries on network / server errors',
							type: 'integer',
							description:
								'The maximum number of times the crawler will retry the request on network, proxy or server errors. If the (n+1)-th request still fails, the crawler will mark this request as failed.',
							minimum: 0,
							maximum: 20,
							default: 3,
						},
						requestTimeoutSecs: {
							title: 'Request timeout',
							type: 'integer',
							description:
								'Timeout in seconds for making the request and processing its response. Defaults to 60s.',
							minimum: 1,
							maximum: 600,
							default: 60,
							unit: 'seconds',
						},
						minFileDownloadSpeedKBps: {
							title: 'Minimum file download speed',
							type: 'integer',
							description:
								'The minimum viable file download speed in kilobytes per seconds. If the file download speed is lower than this value for a prolonged duration, the crawler will consider the file download as failing, abort it, and retry it again (up to "Maximum number of retries" times). This is useful to avoid your crawls being stuck on slow file downloads.',
							default: 128,
							unit: 'kilobytes per second',
						},
						dynamicContentWaitSecs: {
							sectionCaption: 'HTML processing',
							title: 'Wait for dynamic content',
							type: 'integer',
							description:
								"The maximum time in seconds to wait for dynamic page content to load. By default, it is 10 seconds. The crawler will continue processing the page either if this time elapses, or if it detects the network became idle as there are no more requests for additional resources.\n\nWhen using the **Wait for selector** option, the crawler will wait for the selector to appear for this amount of time. If the selector doesn't appear within this period, the request will fail and will be retried.\n\nNote that this setting is ignored for the raw HTTP client, because it doesn't execute JavaScript or loads any dynamic resources. Similarly, if the value is set to `0`, the crawler doesn't wait for any dynamic to load and processes the HTML as provided on load.",
							default: 10,
							unit: 'seconds',
						},
						waitForSelector: {
							title: 'Wait for selector',
							type: 'string',
							description:
								"If set, the crawler will wait for the specified CSS selector to appear in the page before proceeding with the content extraction. This is useful for pages for which the default content load recognition by idle network fails. Setting this option completely disables the default behavior, and the page will be processed only if the element specified by this selector appears. If the element doesn't appear within the **Wait for dynamic content** timeout, the request will fail and will be retried later. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function.\n\nWith the raw HTTP client, this option checks for the presence of the selector in the HTML content and throws an error if it's not found.",
							default: '',
							editor: 'textfield',
						},
						softWaitForSelector: {
							title: 'Soft wait for selector',
							type: 'string',
							description:
								"If set, the crawler will wait for the specified CSS selector to appear in the page before proceeding with the content extraction. Unlike the `waitForSelector` option, this option doesn't fail the request if the selector doesn't appear within the timeout (the request processing will continue).",
							default: '',
							editor: 'textfield',
						},
						maxScrollHeightPixels: {
							title: 'Maximum scroll height',
							type: 'integer',
							minimum: 0,
							description:
								"The crawler will scroll down the page until all content is loaded (and network becomes idle), or until this maximum scrolling height is reached. Setting this value to `0` disables scrolling altogether.\n\nNote that this setting is ignored for the raw HTTP client, because it doesn't execute JavaScript or loads any dynamic resources.",
							default: 5000,
							unit: 'pixels',
						},
						keepElementsCssSelector: {
							title: 'Keep HTML elements (CSS selector)',
							type: 'string',
							description:
								'An optional CSS selector matching HTML elements that should be preserved in the DOM. If provided, all HTML elements which are not matching the CSS selectors or their descendants are removed from the DOM. This is useful to extract only relevant page content. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function. \n\nThis option runs before the `HTML transformer` option. If you are missing content in the output despite using this option, try disabling the `HTML transformer`.',
							editor: 'textarea',
							default: '',
							prefill: '',
						},
						removeElementsCssSelector: {
							title: 'Remove HTML elements (CSS selector)',
							type: 'string',
							description:
								'A CSS selector matching HTML elements that will be removed from the DOM, before converting it to text, Markdown, or saving as HTML. This is useful to skip irrelevant page content. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function. \n\nBy default, the Actor removes common navigation elements, headers, footers, modals, scripts, and inline image. You can disable the removal by setting this value to some non-existent CSS selector like `dummy_keep_everything`.',
							editor: 'textarea',
							default:
								'nav, footer, script, style, noscript, svg, img[src^=\'data:\'],\n[role="alert"],\n[role="banner"],\n[role="dialog"],\n[role="alertdialog"],\n[role="region"][aria-label*="skip" i],\n[aria-modal="true"]',
							prefill:
								'nav, footer, script, style, noscript, svg, img[src^=\'data:\'],\n[role="alert"],\n[role="banner"],\n[role="dialog"],\n[role="alertdialog"],\n[role="region"][aria-label*="skip" i],\n[aria-modal="true"]',
						},
						removeCookieWarnings: {
							title: 'Remove cookie warnings',
							type: 'boolean',
							description:
								"If enabled, the Actor will try to remove cookies consent dialogs or modals, using the [I don't care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension, to improve the accuracy of the extracted text. Note that there is a small performance penalty if this feature is enabled.\n\nThis setting is ignored when using the raw HTTP crawler type.",
							default: true,
						},
						blockMedia: {
							title: 'Block loading of images and videos',
							type: 'boolean',
							description:
								'If the flag is enabled and the Actor is using a headless browser, it will not load images, fonts, stylesheets and videos to improve performance. It will load scripts as usual - that is after all the point of using a headless browser.',
							default: false,
							prefill: true,
						},
						expandIframes: {
							title: 'Expand iframe elements',
							type: 'boolean',
							description:
								'By default, the Actor will extract content from `iframe` elements. If you want to specifically skip `iframe` processing, disable this option. Works only for the `playwright:firefox` crawler type.',
							default: true,
						},
						clickElementsCssSelector: {
							title: 'Expand clickable elements',
							type: 'string',
							editor: 'textfield',
							description:
								'A CSS selector matching DOM elements that will be clicked. This is useful for expanding collapsed sections, in order to capture their text content. The value must be a valid CSS selector as accepted by the `document.querySelectorAll()` function. ',
							default: '[aria-expanded="false"]',
							prefill: '[aria-expanded="false"]',
						},
						stickyContainerCssSelector: {
							title: 'Make containers sticky',
							type: 'string',
							editor: 'textfield',
							description:
								'This is an **experimental** feature. A CSS selector matching DOM elements that will be prevented from deleting any of their children. This is useful in conjunction with the "Expand clickable elements" option on pages where hidden content is actually removed from the DOM (i.e., some variants of the accordion pattern). Enabling this might corrupt the extracted content, which is why it is disabled by default. It is possible to enable the feature for the whole page with the `*` selector, or you can target specific elements if the former has unwanted side effects.',
						},
						htmlTransformer: {
							title: 'HTML transformer',
							type: 'string',
							enum: ['readableTextIfPossible', 'readableText', 'extractus', 'none'],
							enumTitles: [
								'Mozilla Readability with fallback',
								'Mozilla Readability',
								'Extractus',
								'None',
							],
							description:
								"Specify how to transform the HTML to extract meaningful content without any extra fluff, like navigation or modals. The HTML transformation happens after removing and clicking the DOM elements.\n\n- **Readable text with fallback** - Uses Mozilla's Readability to extract the main content but falls back to the original HTML if the page doesn't appear to be an article. This is useful for websites with mixed content types (articles, product pages, etc.) as it preserves more content on non-article pages.\n\n- **Readable text** (default) - Uses Mozilla's Readability to extract the main article content, removing navigation, headers, footers, and other non-essential elements. Works best for article-rich websites and blogs.\n\n- **Extractus** - Uses the Extractus article extraction library, which is an alternative content extraction algorithm. May work better than Readability on certain websites, particularly news sites or blogs with specific layouts.\n\n- **None** - Only applies basic cleaning (removing elements specified via 'Remove HTML elements' option) without any content extraction algorithm. Best when you want to preserve most of the original HTML structure with minimal processing.\n\nYou can examine output of all transformers by enabling the debug mode.\n",
							default: 'readableText',
						},
						readableTextCharThreshold: {
							title: 'Readable text extractor character threshold',
							type: 'integer',
							description:
								'A configuration options for the "Readable text" HTML transformer. It contains the minimum number of characters an article must have in order to be considered relevant.',
							default: 100,
							editor: 'hidden',
						},
						aggressivePrune: {
							title: 'Remove duplicate text lines',
							type: 'boolean',
							description:
								'This is an **experimental feature**. If enabled, the crawler will prune content lines that are very similar to the ones already crawled on other pages, using the Count-Min Sketch algorithm. This is useful to strip repeating content in the scraped data like menus, headers, footers, etc. In some (not very likely) cases, it might remove relevant content from some pages.',
							default: false,
						},
						debugMode: {
							title: 'Debug mode (stores output of all HTML transformers)',
							type: 'boolean',
							description:
								'If enabled, the Actor will store the output of all types of HTML transformers, including the ones that are not used by default, and it will also store the HTML to Key-value Store with a link. All this data is stored under the `debug` field in the resulting Dataset.',
							default: false,
						},
						debugLog: {
							title: 'Debug log',
							type: 'boolean',
							description:
								'If enabled, the actor log will include debug messages. Beware that this can be quite verbose.',
							default: false,
						},
						saveHtml: {
							sectionCaption: 'Output settings',
							title: 'Save HTML to dataset (deprecated)',
							type: 'boolean',
							description:
								"If enabled, the crawler stores full transformed HTML of all pages found to the output dataset under the `html` field. **This option has been deprecated** in favor of the `saveHtmlAsFile` option, because the dataset records have a size of approximately 10MB and it's harder to review the HTML for debugging.",
							default: false,
						},
						saveHtmlAsFile: {
							title: 'Save HTML to key-value store',
							type: 'boolean',
							description:
								"If enabled, the crawler stores full transformed HTML of all pages found to the default key-value store and saves links to the files as `htmlUrl` field in the output dataset. Storing HTML in key-value store is preferred to storing it into the dataset with the `saveHtml` option, because there's no size limit and it's easier for debugging as you can easily view the HTML.",
							default: false,
						},
						saveMarkdown: {
							title: 'Save Markdown',
							type: 'boolean',
							description:
								'If enabled, the crawler converts the transformed HTML of all pages found to Markdown, and stores it under the `markdown` field in the output dataset.',
							default: true,
						},
						saveFiles: {
							title: 'Save files',
							type: 'boolean',
							description:
								'If enabled, the crawler downloads files linked from the web pages, as long as their URL has one of the following file extensions: PDF, DOC, DOCX, XLS, XLSX, and CSV. Note that unlike web pages, the files are downloaded regardless if they are under **Start URLs** or not. The files are stored to the default key-value store, and metadata about them to the output dataset, similarly as for web pages.',
							default: false,
						},
						saveScreenshots: {
							title: 'Save screenshots (headless browser only)',
							type: 'boolean',
							description:
								'If enabled, the crawler stores a screenshot for each article page to the default key-value store. The link to the screenshot is stored under the `screenshotUrl` field in the output dataset. It is useful for debugging, but reduces performance and increases storage costs.\n\nNote that this feature only works with the `playwright:firefox` crawler type.',
							default: false,
						},
						maxResults: {
							title: 'Max results',
							type: 'integer',
							description:
								'The maximum number of resulting web pages to store. The crawler will automatically finish after reaching this number. This setting is useful to prevent accidental crawler runaway. If both **Max pages** and **Max results** are defined, then the crawler will finish when the first limit is reached. Note that the crawler skips pages with the canonical URL of a page that has already been crawled, hence it might crawl more pages than there are results.',
							minimum: 0,
							default: 9999999,
						},
						textExtractor: {
							title: 'Text extractor (deprecated)',
							type: 'string',
							description:
								'Removed in favor of the `htmlTransformer` option. Will be removed soon.',
							editor: 'hidden',
						},
						clientSideMinChangePercentage: {
							title: '(Adaptive crawling only) Minimum client-side content change percentage',
							description:
								'The least amount of content (as a percentage) change after the initial load required to consider the pages client-side rendered',
							type: 'integer',
							minimum: 1,
							editor: 'hidden',
							default: 15,
						},
						renderingTypeDetectionPercentage: {
							title:
								'(Adaptive crawling only) How often should the crawler attempt to detect page rendering type',
							description: 'How often should the adaptive attempt to detect page rendering type',
							type: 'integer',
							minimum: 1,
							maximum: 100,
							editor: 'hidden',
							default: 10,
						},
					},
					required: ['startUrls', 'proxyConfiguration'],
				},
				dockerfile: './Dockerfile',
				storages: {
					dataset: {
						actorSpecification: 1,
						title: 'Website Content Crawler Dataset',
						description: '',
						views: {
							default: {
								title: 'Text',
								description: 'View of URLs of web pages and their content as simple plain text.',
								transformation: {
									fields: ['url', 'text'],
								},
								display: {
									component: 'table',
									properties: {
										url: {
											label: 'Webpage URL',
										},
										text: {
											label: 'Extracted text',
										},
									},
								},
							},
							markdown: {
								title: 'Markdown',
								description:
									'View of URLs of web pages and their content as Markdown with formatting.',
								transformation: {
									fields: ['url', 'markdown'],
								},
								display: {
									component: 'table',
									properties: {
										url: {
											label: 'Webpage URL',
										},
										markdown: {
											label: 'Extracted Markdown',
										},
									},
								},
							},
						},
					},
				},
				readme:
					'Website Content Crawler is an [Apify Actor](https://docs.apify.com/platform/actors) that can perform\na deep crawl of one or more websites and extract text content from the web pages.\nIt is useful to download data from websites such as documentation,\nknowledge bases, help sites, or blogs.\n\nThe Actor was specifically designed to extract data for feeding, fine-tuning, or training\nlarge language models (LLMs) such as [GPT-4](https://openai.com/product/gpt-4), [ChatGPT](https://openai.com/blog/chatgpt),\nor [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/).\n\nWebsite Content Crawler has a simple input configuration so that it can be easily integrated into customer-facing products, where customers\ncan enter just a URL of the website they want to have indexed by an AI application.\nYou can retrieve the results using the API to formats such as JSON or CSV,\nwhich can be fed directly to your LLM,\n[vector database](https://blog.apify.com/what-is-a-vector-database/), or directly to ChatGPT.\n\n\n## Main features\n\nWebsite Content Crawler is built upon [Crawlee](https://crawlee.dev/), Apify\'s state-of-the-art\nopen-source library for web scraping and crawling. The Actor can:\n\n- Crawl JavaScript-enabled websites using **headless Firefox** or simple sites using **raw HTTP**.\n- Circumvent **anti-scraping protections** using browser fingerprinting and proxies.\n- Save web content in plain text, **Markdown**, or HTML.\n- Crawl **pages behind a login**.\n- **Download files** in PDF, DOC, DOCX, XLS, XLSX, or CSV formats.\n- **Remove fluff** from pages like navigation, header, footers, ads, modals, or cookies warnings to improve the accuracy of the data.\n- Load content of pages with **infinite scroll**.\n- Use sitemaps to find more URLs on the website.\n- **Scale gracefully** from tiny sites to sites with millions of pages by leveraging the Apify platform capabilities.\n- Integrate with **ðŸ¦œðŸ”—LangChain**,: **LlamaIndex**, **Haystack**, **Pinecone**, **Qdrant**, or **OpenAI Assistant**\n- and much more...\n\nLearn about the key features and capabilities in the **Website Content Crawler Overview** video:\n\n[Introducing Website Content Crawler](https://www.youtube.com/watch?v=vUMPfIOfXXQ)\n\nStill unsure if the Website Content Crawler can handle your use case? Simply try it for free and see the results for yourself.\n\n\n## Designed for generative AI and LLMs\n\nThe results of Website Content Crawler can help you feed, fine-tune or train your large language models (LLMs)\nor provide context for prompts for ChatGPT.\nIn return, the model will answer questions based on your or your customer\'s websites and content.\n\nTo learn more, check out our **Web Scraping Data for Generative AI** video on this topic, showcasing the Website Content Crawler:\n\n[Web Scraping Data for Generative AI webinar](https://www.youtube.com/watch?v=8uvHH-ocSes)\n\n**Custom chatbots for customer support**\n\nCustomer service chatbots personalized on customer websites, such as documentation or knowledge bases,\nare one of the most promising use cases of AI and LLMs. Let your\ncustomers easily onboard by typing the URL of their site, and thus give your chatbot detailed\nknowledge of their product or service.\nLearn more about this use case in our [blog post](https://blog.apify.com/talk-to-your-website-with-large-language-models/).\n\n**Generate personalized content based on customerâ€™s copy**\n\nChatGPT and LLMs can write articles for you, but they wonâ€™t sound like you wrote them. Feed all your old blogs into your\nmodel to make it sound like you. Alternatively, train the model on your customersâ€™ blogs and have it write in their tone of voice.\nOr help their technical writers with making first drafts of new documentation pages.\n\n**Retrieval Augmented Generation (RAG) use cases**\n\nUse your website content to create an all-knowing AI assistant. The LLM-powered bot can then answer questions based on your website content,\nor even generate new content based on the existing one. This is a great way to provide a personalized experience to your customers\nor help your employees find the information they need faster.\n\n**Summarization, translation, proofreading at scale**\n\nGot some old docs or blogs that need to be improved? Use Website Content Crawler to scrape the content, feed it to the ChatGPT API,\nand ask it to summarize, proofread, translate, or change the style of the content.\n\n**Enhance your custom GPTs**\n\nUploading knowledge files gives custom OpenAI GPTs reliable information to refer to when generating answers. With Website Content Crawler, you can scrape data from any website to [provide your GPT with custom knowledge](https://blog.apify.com/custom-gpts-knowledge/).\n\n\n## How does it work?\n\nWebsite Content Crawler operates in three stages:\n\n1) **Crawling** - Finds and downloads the right web pages.\n2) **HTML processing** - Transforms the DOM of crawled pages to e.g. remove navigation, header, footer, cookie warnings, and other fluff.\n3) **Output** - Converts the resulting DOM to plain text or Markdown and saves downloaded files.\n\nFor clarity, the input settings of the Actor are organized according to the above stages. Note that input settings\nhave reasonable defaultsâ€”the only mandatory setting is the **Start URLs**.\n\n### Crawling\n\nWebsite Content Crawler only needs one or more **Start URLs** to run, typically the top-level URL of the documentation site, blog, or\nknowledge base that you want to scrape. The actor crawls the start URLs, finds links to other pages,\nand recursively crawls those pages, too, as long as their URL is under the start URL.\n\nFor example, if you enter the start URL `https://example.com/blog/`, the\nactor will crawl pages like `https://example.com/blog/article-1` or `https://example.com/blog/section/article-2`,\nbut will skip pages like `https://example.com/docs/something-else`.\n\nYou can also force the crawler to skip certain URLs using the **Exclude URLs (globs)** input setting,\nwhich specifies an array of glob patterns matching URLs of pages to be skipped.\nNote that this setting affects only links found on pages, but not **Start URLs**, which are always crawled.\nFor example, `https://{store,docs}.example.com/**` will exclude all URLs starting with\n`https://store.example.com/` and `https://docs.example.com/`.\nOr `https://example.com/**/*\\?*foo=*` exclude all URLs that contain `foo` query parameter with any value.\nYou can learn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fscrape_this).\n\nThe Actor automatically skips duplicate pages identified by the same [canonical URL](https://en.wikipedia.org/wiki/Canonical_link_element);\nthose pages are loaded and counted towards the _Max pages_ limit but not saved to the results.\n\nWebsite Content Crawler provides various input settings to customize the crawling.\nFor example, you can select the crawler type:\n- **Adaptive switching between browser and raw HTTP client** - The crawler automatically switches between raw HTTP for static pages and Firefox browser (via Playwright) for dynamic pages, to get the maximum performance wherever possible.\n- **Headless web browser** - Useful for modern websites with\n  anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and\n  automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it\n  requires more computing resources and is slower.\n- **Stealthy web browser** (default) - Another headless web browser, but with anti-blocking measures enabled. Try this if you encounter\n  bot protection while scraping. For best performance, use it with Apify Proxy.\n- **Raw HTTP client** - High-performance crawling mode that uses raw HTTP requests to fetch the pages.\n  It is faster and cheaper, but it might not work on all websites.\n- **Raw HTTP client with JS execution (JSDOM)** [experimental] - A compromise between a browser and raw HTTP crawlers. Good performance and should work on almost all websites, including those with dynamic content.\n  However, it is still experimental and might sometimes crash, so we don\'t recommend it in production settings yet.\n\nYou can also set additional input parameters such as a maximum number of pages, maximum crawling depth,\nmaximum concurrency, proxy configuration, timeout, etc., to control the behavior and performance of the Actor.\n\n### HTML processing\n\nThe goal of the HTML processing step is to ensure each web page has the right content â€” neither less nor more.\n\nIf you\'re using a headless browser **Crawler type**,\nwhenever a web page is loaded,\nthe Actor can wait a certain time or scroll to a certain height\nto ensure all dynamic page content is loaded, using the **Wait for dynamic content** or **Maximum scroll height** input settings, respectively.\nIf **Expand clickable elements** is enabled, the Actor tries to click various DOM\nelements to ensure their content is expanded and visible in the resulting text.\n\nOnce the web page is ready, the Actor\ntransforms its DOM to remove irrelevant content in order to help you ensure you\'re feeding your AI models with relevant data\nto keep them accurate.\n\nFirst, the Actor removes DOM nodes matching the **Remove HTML elements (CSS selector)**. The provided default value attempts\nto remove all common types of modals, navigation, headers, or footers, as well as scripts and inline images\nto reduce the output HTML size.\n\nIf the **Extract HTML elements (CSS selector)** option is specified, the Actor only keeps the contents of the elements targeted by this CSS selector and removes all the other HTML elements from the DOM.\n\nThen, if **Remove cookie warnings** is enabled,\nthe Actor removes cookie warnings using the [I don\'t care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension.\n\nFinally, the Actor transforms the page using the selected **HTML transformer**, whose goal is to only keep the important content of the page and reduce\nits complexity before converting it to text. Basically, to keep just the "meat" of the article or a page.\n\n### File download\n\nIf the `Save files` option is set, the Actor will download "document" files linked from the page. This is limited to PDF, DOC, DOCX, XLS, and XLSX files.\n\nNote that these files are exempt from the URL scoping rules - any file linked from the page will be downloaded, regardless of its URL.\nYou can change this behaviour by using the `Include / Exclude URLs (globs)` setting.\n\nThe hard limit for a single file download time is 1 hour. If the download of a single file takes longer, the Actor will abort the download.\n\nFurthermore, you can specify the minimal file download speed in kilobytes per second. If the download speed stays below this threshold for more than 10 seconds, the download will be aborted.\nThis is configurable via the `Minimal file download speed (KB/s)` setting.\n\n### Output\n\nOnce the web page HTML is processed, the Actor converts it to the desired output format, including plain text, Markdown to preserve rich formatting,\nor save the full HTML or a screenshot of the page, which is useful for debugging.\nThe Actor also saves important metadata about the content, such as author, language, publishing date, etc.\n\nThe results of the actor are stored in the default [Dataset](https://docs.apify.com/platform/storage/dataset) associated\nwith the Actor run, from where you can access it via API and export to formats like JSON, XML, or CSV.\n\n\n## Example\n\nThis example shows how to scrape all pages from the Apify documentation at https://docs.apify.com/:\n\n### Input\n\n![input-screenshot.png](https://apify-uploads-prod.s3.amazonaws.com/d9f0eb8b-ae01-42be-8629-39e659a14ed6_website_content_crawler_input_example.png)\n\n[See full input](https://apify.com/apify/website-content-crawler/input-schema) with description.\n\n### Output\n\nThis is how one crawled page (https://docs.apify.com/academy/web-scraping-for-beginners) looks in a browser:\n\n![page-screenshot.png](https://apify-uploads-prod.s3.amazonaws.com/399ddefd-3877-41e7-86ed-4025cdea46f8_Screenshot2023-03-29at10.56.57.png)\n\nAnd here is how the crawling result looks in JSON format (note that other formats like CSV or Excel are also supported).\nThe main page content can be found in the `text` field, and it only contains the valuable\ncontent, without menus and other noise:\n\n```json\n{\n    "url": "https://docs.apify.com/academy/web-scraping-for-beginners",\n    "crawl": {\n        "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",\n        "loadedTime": "2023-04-05T16:26:51.030Z",\n        "referrerUrl": "https://docs.apify.com/academy",\n        "depth": 0\n    },\n    "metadata": {\n        "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",\n        "title": "Web scraping for beginners | Apify Documentation",\n        "description": "Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.",\n        "author": null,\n        "keywords": null,\n        "languageCode": "en"\n    },\n    "screenshotUrl": null,\n    "text": "Skip to main content\\nOn this page\\nWeb scraping for beginners\\nLearn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.\\nWelcome to Web scraping for beginners, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you\'re looking for a quick start, we recommend trying this tutorial instead.\\nThis course is made by Apify, the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you\'ll be able to run your scrapers on any computer. No Apify account needed.\\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the Apify platform course, where we\'ll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\\nWhy learn scraper development?â€‹\\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technology like React or Next.js will be a piece of cake.\\nCourse Summaryâ€‹\\nWhen we set out to create the Academy, we wanted to build a complete guide to modern web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\\nThis is what you\'ll learn in the Web scraping for beginners course:\\nWeb scraping for beginners\\nBasics of data extraction\\nBasics of crawling\\nBest practices\\nRequirementsâ€‹\\nYou don\'t need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don\'t be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you\'re new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using [] instead of () can make a lot of difference.\\nIf you don\'t already have basic programming knowledge and would like to be well-prepared for this course, we recommend taking a JavaScript course and learning about CSS Selectors.\\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\\nIdeally, you should have at least a moderate understanding of the following concepts:\\nJavaScript + Node.jsâ€‹\\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and async...await), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\\nasync...await (YouTube)\\nJavaScript loops (MDN)\\nModularity in Node.js\\nGeneral web developmentâ€‹\\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because the knowledge of them will be assumed (unless we\'re showing something out of the ordinary).\\nHTML\\nHTTP protocol\\nDevTools\\njQuery or Cheerioâ€‹\\nWe\'ll be using the Cheerio package a lot to parse data from HTML. This package provides a simple API using jQuery syntax to help traverse downloaded HTML within Node.js.\\nNext upâ€‹\\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. So let\'s get to it!\\nIf you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the Basics of crawling section.\\nWhy learn scraper development?\\nCourse Summary\\nRequirements\\nJavaScript + Node.js\\nGeneral web development\\njQuery or Cheerio\\nNext up",\n    "html": null,\n    "markdown": "  Web scraping for beginners | Apify Documentation       \\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)\\n\\nOn this page\\n\\n# Web scraping for beginners\\n\\n**Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.**\\n\\n* * *\\n\\nWelcome to **Web scraping for beginners**, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you\'re looking for a quick start, we recommend trying [this tutorial](https://blog.apify.com/web-scraping-javascript-nodejs/) instead.\\n\\nThis course is made by [Apify](https://apify.com), the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you\'ll be able to run your scrapers on any computer. No Apify account needed.\\n\\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the [Apify platform course](/academy/apify-platform), where we\'ll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\\n\\n## Why learn scraper development?[â€‹](#why-learn \\"Direct link to Why learn scraper development?\\")\\n\\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\\n\\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\\n\\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technology like React or Next.js will be a piece of cake.\\n\\n## Course Summary[â€‹](#summary \\"Direct link to Course Summary\\")\\n\\nWhen we set out to create the Academy, we wanted to build a complete guide to modern web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\\n\\nThis is what you\'ll learn in the **Web scraping for beginners** course:\\n\\n*   [Web scraping for beginners](/academy/web-scraping-for-beginners)\\n    *   [Basics of data extraction](/academy/web-scraping-for-beginners/data-collection)\\n    *   [Basics of crawling](/academy/web-scraping-for-beginners/crawling)\\n    *   [Best practices](/academy/web-scraping-for-beginners/best-practices)\\n\\n## Requirements[â€‹](#requirements \\"Direct link to Requirements\\")\\n\\nYou don\'t need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don\'t be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you\'re new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using `[]` instead of `()` can make a lot of difference.\\n\\n> If you don\'t already have basic programming knowledge and would like to be well-prepared for this course, we recommend taking a [JavaScript course](https://www.codecademy.com/learn/introduction-to-javascript) and learning about [CSS Selectors](https://www.w3schools.com/css/css_selectors.asp).\\n\\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\\n\\nIdeally, you should have at least a moderate understanding of the following concepts:\\n\\n### JavaScript + Node.js[â€‹](#javascript-and-node \\"Direct link to JavaScript + Node.js\\")\\n\\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and `async...await`), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\\n\\n*   [`async...await` (YouTube)](https://www.youtube.com/watch?v=vn3tm0quoqE&ab_channel=Fireship)\\n*   [JavaScript loops (MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Loops_and_iteration)\\n*   [Modularity in Node.js](https://www.section.io/engineering-education/how-to-use-modular-patterns-in-nodejs/)\\n\\n### General web development[â€‹](#general-web-development \\"Direct link to General web development\\")\\n\\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because the knowledge of them will be **assumed** (unless we\'re showing something out of the ordinary).\\n\\n*   [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML)\\n*   [HTTP protocol](https://developer.mozilla.org/en-US/docs/Web/HTTP)\\n*   [DevTools](/academy/web-scraping-for-beginners/data-collection/browser-devtools)\\n\\n### jQuery or Cheerio[â€‹](#jquery-or-cheerio \\"Direct link to jQuery or Cheerio\\")\\n\\nWe\'ll be using the [**Cheerio**](https://www.npmjs.com/package/cheerio) package a lot to parse data from HTML. This package provides a simple API using jQuery syntax to help traverse downloaded HTML within Node.js.\\n\\n## Next up[â€‹](#next \\"Direct link to Next up\\")\\n\\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. So [let\'s get to it!](/academy/web-scraping-for-beginners/introduction)\\n\\n> If you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the [Basics of crawling](/academy/web-scraping-for-beginners/crawling) section.\\n\\n*   [Why learn scraper development?](#why-learn)\\n*   [Course Summary](#summary)\\n*   [Requirements](#requirements)\\n    *   [JavaScript + Node.js](#javascript-and-node)\\n    *   [General web development](#general-web-development)\\n    *   [jQuery or Cheerio](#jquery-or-cheerio)\\n*   [Next up](#next)"\n}\n```\n\n## Integration with the AI ecosystem\n\nThanks to the native [Apify platform integrations](https://docs.apify.com/platform/integrations),\nWebsite Content Crawler can seamlessly connect with various third-party\nsystems and tools.\n\n### Exporting GPT knowledge files\n\nApify allows you to seamlessly export the results of Website Content Crawler runs to your custom GPTs.\n\nTo do this, go to the Output tab of the Actor run and click the "Export results" button. From here, pick `JSON` and click "Export". You can then upload the JSON file to your custom GPTs.\n\nFor a step-by-step guide, see [How to add a knowledge base to your GPTs](https://blog.apify.com/custom-gpts-knowledge/#how-to-add-knowledge-to-gpts-step-by-step-guide).\n\n### LangChain integration\n\n[LangChain](https://github.com/hwchase17/langchain) is the most popular framework for\ndeveloping applications powered by language models.\nIt provides an [integration for Apify](https://python.langchain.com/en/latest/modules/agents/tools/examples/apify.html),\nso you can feed Actor results directly to LangChainâ€™s vector databases,\nenabling you to easily create ChatGPT-like query interfaces to\nwebsites with documentation, knowledge base, blog, etc.\n\n\n#### Python example\n\nFirst, install LangChain with OpenAI LLM and Apify API client for Python:\n\n```bash\npip install apify-client langchain langchain_community langchain_openai openai tiktoken\n```\n\nAnd then create a ChatGPT-powered answering machine:\n\n```python\nimport os\n\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_community.utilities import ApifyWrapper\nfrom langchain_core.document_loaders.base import Document\nfrom langchain_openai import OpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# Set up your Apify API token and OpenAI API key\nos.environ["OPENAI_API_KEY"] = "Your OpenAI API key"\nos.environ["APIFY_API_TOKEN"] = "Your Apify API token"\n\napify = ApifyWrapper()\n\n# Run the Website Content Crawler on a website, wait for it to finish, and save its results into a LangChain document loader:\nloader = apify.call_actor(\n    actor_id="apify/website-content-crawler",\n    run_input={"startUrls": [{"url": "https://docs.apify.com/"}], "maxCrawlPages": 10},\n    dataset_mapping_function=lambda item: Document(\n        page_content=item["text"] or "", metadata={"source": item["url"]}\n    ),\n)\n# Initialize the vector database with the text documents:\nindex = VectorstoreIndexCreator(embedding=OpenAIEmbeddings()).from_loaders([loader])\n\n# Finally, query the vector database:\nquery = "What is Apify?"\nresult = index.query_with_sources(query, llm=OpenAI())\n\nprint("answer:", result["answer"])\nprint("source:", result["sources"])\n```\n\nThe query produces an answer like this:\n\n> _Apify is a platform for developing, running, and sharing serverless cloud programs. It enables users to create web scraping and automation tools and publish them on the Apify platform._\n>\n> https://docs.apify.com/platform/actors, https://docs.apify.com/platform/actors/running/actors-in-store, https://docs.apify.com/platform/security, https://docs.apify.com/platform/actors/examples\n\nFor details and Jupyter notebook, see [Apify integration for LangChain](https://python.langchain.com/docs/integrations/tools/apify/).\n\n#### Node.js example\n\nSee [detailed example](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/web_loaders/apify_dataset) in LangChain for JavaScript.\n\n### LlamaIndex integration\n\n[LlamaIndex](https://docs.llamaindex.ai/) is a Python library that provides a central interface to connect LLMs with external data.\nThe [Apify integration](https://llamahub.ai/l/readers/llama-index-readers-apify?from=) makes it easy to feed LlamaIndex applications with data crawled from the web.\n\nInstall all required packages:\n\n```bash\npip install apify-client llama-index-core llama-index-readers-apify\n```\n\n```python\nfrom llama_index.core import Document\nfrom llama_index.readers.apify import ApifyActor\n\nreader = ApifyActor("<My Apify API token>")\n\ndocuments = reader.load_data(\n    actor_id="apify/website-content-crawler",\n    run_input={\n        "startUrls": [{"url": "https://docs.llamaindex.ai/en/latest/"}]\n    },\n    dataset_mapping_function=lambda item: Document(\n        text=item.get("text"),\n        metadata={\n            "url": item.get("url"),\n        },\n    ),\n)\n```\n\n### Vector database integrations (Pinecone, Qdrant)\n\nWebsite Content Crawler can be easily integrated with vector databases to store the crawled data for semantic search.\nUsing Apify\'s [Pinecone](https://apify.com/apify/pinecone-integration) or [Qdrant](https://apify.com/apify/qdrant-integration) integration Actors, you can upload the results of Website Content Crawler directly into a vector database.\nThe integrations support incremental updates, updating only the data that has changed since the last crawl.\nThis helps to reduce costly embedding computation and storage operations, making it suitable for regular updates of large websites.\nJust set up the Pinecone integration Actor with Website Content Crawler using this [step-by-step guide](https://docs.apify.com/platform/integrations/pinecone).\n\n### GPT integration\n\nYou can use Website Content Crawler to add knowledge to your GPTs. Crawl a website and upload the scraped dataset to your custom GPT.\nThe video tutorial below demonstrates how it works.\n\n[How to add a knowledge base to your GPTs](https://www.youtube.com/watch?v=z552gt-3Ce0)\n\nYou can also use the Website Content Crawler together with the OpenAI Assistant to update its knowledge base with web content using the [OpenAI VectorStore Integration](https://apify.com/jiri.spilka/openai-vector-store-integration).\n\n\n## How much does it cost?\n\nWebsite Content Crawler is free to useâ€”you only pay for the Apify platform usage consumed by the Actor.\nThe exact price depends on the crawler type and settings, website complexity, network speed,\nand random circumstances.\n\nThe main cost driver of Website Content Crawler is the compute power, which is measured in the Actor compute units (CU):\n1 CU corresponds to an actor with 1 GB of\nmemory running for 1 hour. With the baseline price of $0.25/CU, from our tests, the actor usage costs **approximately**:\n\n- $0.5 - $5 per 1,000 web pages with a headless browser, depending on the website\n- $0.2 per 1,000 web pages with raw HTTP crawler\n\nNote that Apify\'s free plan gives you $5 free credits every month and access to [Apify Proxy](https://apify.com/proxy),\nwhich is sufficient for testing and low-volume use cases.\n\n## Troubleshooting\n\n- The Actor works best for crawling sites with multiple URLs. For **extracting text or Markdown from a single URL**,\n  you might prefer to use [RAG Web Browser](https://apify.com/apify/rag-web-browser) in the Standby mode,\n  which is much faster and more efficient.\n- If the **extracted text doesnâ€™t contain the expected page content**, try to select another _Crawler type_.\n  Generally, a headless browser will extract more text as it loads dynamic page content\n  and is less likely to be blocked.\n- If the **extracted text has more than expected page content** (e.g. navigation or footer),\n  try to select another _HTML transformer_, or use the _Remove HTML elements_ setting\n  to skip unwanted parts of the page.\n- If the **crawler is too slow**, try increasing the Actor memory and/or the _Initial concurrency_ setting.\n  Note that if you set the concurrency too high, the Actor will run out of memory and crash,\n  or potentially overload the target site.\n- If the target website is blocking the crawler, make sure to use the **Stealthy web browser (Firefox+Playwright)**\n  crawler type and use residential proxies\n- The crawler **automatically restarts on crash**, and continues where it left off.\n  But if it crashes more than 3 times per minute, the system fails the Actor run.\n\n## Help & support\n\nWebsite Content Crawler is under active development.\nIf you have any feedback or feature ideas, please [submit an issue](https://console.apify.com/actors/aYG0l9s7dbB7j3gbS/issues).\n\n## Is it legal?\n\nWeb scraping is generally legal if you scrape publicly available non-personal data. What you do with the data is another question.\nDocumentation, help articles, or blogs are typically protected by copyright, so you can\'t republish the content without the owner\'s permission.\n\nLearn more about the legality of web scraping in this\n[blog post](https://blog.apify.com/is-web-scraping-legal/). If you\'re not sure, please seek professional legal advice.\n',
				changelog:
					'# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n\n## 0.3.67 (2025-06-19)\n\n### ðŸš€ Features\n\n- Lower default `maxRequestRetries` limit to `3` (#419)\n- Add `ignoreHttpsErrors` input option (#422)\n- Add a flag to disable image and video loading in headless browser (#423)\n\n### ðŸ› Bug Fixes\n\n- `keepElementsCssSelector` doesn&#x27;t affect link enqueuing (#413)\n- Clean up after `SitemapRequestList` to avoid excessive timeouts (#415)\n\n\n\n## 0.3.66 (2025-05-26)\n\n### ðŸš€ Features\n\n- Use code from the Ghostery adblocker to remove cookie modals (#403)\n\n### ðŸ› Bug Fixes\n\n- Only use titles from the document head for the metadata section (#401)\n- Store downloaded HTML explicitly as UTF-8 (#410)\n\n### âš¡ Performance\n\n- Add more &quot;unparseable&quot; file types (`jpg`, `png`, `gif`...) (#402)\n\n\n\n## 0.3.65 (2025-04-17)\n\n### ðŸš€ Features\n\n- Allow toggling the crawlee respectRobotsTxtFile via input (#392)\n- Sticky containers (#390)\n\n### ðŸ› Bug Fixes\n\n- Fix SitemapRequestList persistence (#385)\n\n\n\n## 0.3.64 (2025-03-21)\n\n### ðŸš€ Features\n\n- Handle more types of cookie modals (#378)\n- Store original file name and size for downloads (#377)\n\n\n\n## 0.3.63 (2025-03-17)\n\n### ðŸš€ Features\n\n- Use RequestQueueV1 (#376)\n\n\n\n##  (2025-03-14)\n\n### ðŸ› Bug Fixes\n\n- Automatically close cookie modals by usercentrics.eu (#373)\n\n\n\n## 0.3.61 (2025-03-04)\n\n### ðŸ› Bug Fixes\n\n- Generate unique KVS keys for `saveHtmlAsFile` results (#369)\n- Correctly validate `waitForSelector` input option (#370)\n\n\n\n## 0.3.60 (2025-02-21)\n\n### ðŸš€ Features\n\n- Add support for `softWaitForSelector` (#365)\n\n### ðŸ› Bug Fixes\n\n- More stable crawler stopping (#366)\n\n\n\n## 0.3.59 (2025-01-22)\n\n### ðŸ› Bug Fixes\n\n- Work around missing `content-type` header in `FileDownload` (#360)\n- Extract only top-level `title` (skip iframes) (#362)\n\n\n\n## 0.3.58 (2025-01-10)\n\n### ðŸš€ Features\n\n- Push `latest` to Apify with GH actions, add auto changelog bumps (#347)\n- More lenient behaviour on missing `content-type` response header (#353)\n- Improve antiblocking performance (#354)\n\n\n## 0.3.57 (2024-12-13)\n\nThis release only contains documentation changes in readme and input schema.\n\n## 0.3.56 (2024-11-25)\n- Input:\n    - Empty `includeUrlGlobs` are now filtered out with a warning log message. To enforce the old behavior (i.e. matching everything), use `**` instead.\n- Behaviour:\n    - The Actor now automatically drops the request queue associated with the file download.\n    - Only the first `<title>` element on the page is prepended to the exported content.\n    - The crawler now uses the correct scope with all `startUrls` being sitemaps.\n    - Sitemaps are now processed in a separate thread. The 30 seconds per sitemap request limit is now strongly enforced.\n\n## 0.3.55 (2024-11-11)\n- Behaviour:\n    - The sitemap timeout warning is only logged the first time.\n\n## 0.3.54 (2024-11-07)\n- Input:\n    - The default `removeElementsCssSelector` now removes `img` elements with `data:` URLs to prevent cluttering the text output.\n\n- Behaviour:\n    - `expandIframes` now skips broken `iframe` elements instead of failing the whole request.\n    - Actor now parses formatted (indented or newline-separated) sitemaps correctly.\n    - The sitemap discovery process is now parallelized. Logging is improved to show the progress of the sitemap discovery.\n    - Sitemap processing now has stricter per-URL time limits to prevent indefinite hangs.\n\n## 0.3.53 (2024-10-22)\n- Input:\n    - New `keepElementsCssSelector` accepts a CSS selector targetting elements to extract from the page to the output.\n- Behaviour:\n    - Actor optimizes RQ writes by following the `maxCrawlPages` limits better.\n\n## 0.3.52 (2024-10-10)\n- Behavior:\n    - Handle sitemap-based requests correctly. Solves the indefinite `RequestQueue` hanging read issue.\n\n## 0.3.51 (2024-10-10)\n- Behavior:\n    - Revert internal library update to mitigate the indefinite `RequestQueue` hanging read issue.\n\n## 0.3.50 (2024-10-07)\n- Behavior:\n    - Actor terminates sitemap loading prematurely in case of a timeout.\n    - Sitemap loading now respects `maxRequestRetries` option.\n\n## 0.3.49 (2024-09-23)\n- Behavior:\n    - Use the correct proxy settings when loading sitemap files.\n    - Mitigate sitemap persistence issues with premature stopping on `maxRequests` (`ERR_STREAM_PUSH_AFTER_EOF`).\n\n## 0.3.48 (2024-09-10)\n- Input:\n    - `useSitemaps` option is now pre-filled to `true` to automatically enable it for new users and in API examples.\n\n## 0.3.47 (2024-09-04)\n- Behaviour:\n    - Use crawlee 3.11.3 which should help with the crawler being stuck because of some stale requests in the queue.\n\n## 0.3.46 (2024-08-30)\n- Behaviour:\n    - Process markdown in a separate worker thread so it won\'t block the main process on too large pages.\n    - Sitemap loading with `useSitemaps` doesn\'t block indefinitely on large sitemaps anymore.\n\n## 0.3.45 (2024-08-20)\n- Input:\n    - New `keepFragmentUrls` (URL `#fragments` identify unique pages) input option to consider fragment URLs as separate pages.\n- Behaviour:\n    - Ensure canonical URL is only taken from the main page and not embedded pages from `<iframe>`.\n    - Too large dataset items used to fail, now we retry with a trimmed payload (`html`, `text` and `markdown` fields are trimmed to the first three million characters each).\n\n## 0.3.44 (2024-07-30)\n- Behaviour:\n    - `waitForSelector` option allows users to specify a CSS selector to wait for before extracting the page content. This is useful for pages that load content dynamically and break the automatic waiting mechanism.\n\n## 0.3.43 (2024-07-24)\n- Behaviour:\n    - Change the shadow DOM expansion logic to handle edge cases better.\n\n## 0.3.42 (2024-07-12)\n- Behaviour:\n    - Fix edge cases with the improved `startUrls` sanitization.\n\n## 0.3.41 (2024-07-11)\n- Behaviour:\n    - Better input URLs sanitization to prevent issues with the `startUrls` input.\n\n## 0.3.40 (2024-07-10)\n- Input:\n    - New `expandIframes` (Expand iframe elements) option for extracting content from on-page `iframe` elements. Available only in `playwright:firefox`.\n\n## 0.3.39 (2024-06-28)\n- Behaviour:\n    - Mitigating excessive Request Queue writes in some cases.\n\n## 0.3.38 (2024-06-25)\n- Behaviour:\n    - The `saveScreenshots` option now correctly prints warnings with crawler types that don\'t support screenshots.\n    - The screenshot KVS key now contains the website hostname and a hash of the original URL to avoid collisions.\n\n## 0.3.37 (2024-06-17)\n- Behaviour:\n    - The Actor now respects the advanced request configuration passed through the `Start URLs` input.\n\n## 0.3.36 (2024-06-10)\n- Input:\n    - New `saveHtmlAsFile` option is available which enables storing the HTML into a key-value store, replacing their values in the dataset with links to make the dataset value smaller, since there is a hard limit for its size.\n    - Deprecated `saveHtml` in favor of `saveHtmlAsFile`.\n- Output:\n    - The new `saveHtmlAsFile` option saves the URL under a new `htmlUrl` key in the dataset.\n- Behaviour:\n    - HTML processors don\'t block the main thread and can safely time out.\n    - Better fallback logic for the HTML processing pipeline.\n    - When pushing to dataset, we now detect too large payload and skip retrying (while suggesting to use the new `saveHtmlAsFile` option to get around this problem).\n\n## 0.3.35 (2024-05-23)\n- Behaviour:\n    - `RequestQueue` race condition fixed.\n- Output:\n    - The Readable text extractor now correctly handles the article titles.\n\n## 0.3.34 (2024-05-17)\n- Behaviour:\n    - If any of the Start URLs lead to a sitemap file, it is processed and the links are enqueued.\n    - Performance / QoL improvements (see [Crawlee 3.10.0 changelog](https://github.com/apify/crawlee/releases/tag/v3.10.0) for more details)\n\n## 0.3.33 (2024-04-22)\n- Input:\n    - `AdaptiveCrawler` is the new default (prefill) crawler type.\n- Behaviour:\n    - The use of Chrome + Chromium browsers was deprecated. The Actor now uses only Firefox internally for the browser-based crawlers.\n    - Reimplementation of the file download feature for better stability and performance.\n    - On smaller websites, the `AdaptiveCrawler` skips the adaptive scanning to speed up the crawl.\n\n## 0.3.32 (2024-03-28)\n- Output:\n    - The Actor now stores `metadata.headers` with the HTTP response headers of the crawled page.\n\n## 0.3.31 (2024-03-14)\n- Input:\n    - Invalid `startUrls` are now filtered out and don\'t cause the actor to fail.\n\n## 0.3.30 (2024-02-24)\n- Behavior:\n    - File download now respects `excludeGlobs` and `includeGlobs` input options, stores filenames, and understands `Content-Disposition` HTTP headers (i.e. "forced download").\n- Output:\n    - Better `og:` metatags coverage (`article:`, `movie:` etc.).\n    - Stores JSON-LD metatags in the `metadata` field.\n\n## 0.3.29 (2024-02-05)\n- Input:\n    - `useSitemaps` toggle for sitemap discovery - leads to more consistent results, scrapes also unreachable webpages.\n    - Do not fail on empty globs in input (ignore them instead).\n    - Experimental `playwright:adaptive` crawling mode\n- Output:\n    - Add `metadata.openGraph` output field for contents of `og:*` metatags.\n\n## 0.3.27 (2024-01-25)\n- Input:\n    - `maxRequestRetries` input option for limiting the number of request retries on network, server or parsing errors.\n- Behavior:\n    - Allow large lists of start URLs with deep crawling (`maxCrawlDepth > 0`), as the memory overflow issue from `0.3.18` is now fixed.\n\n## 0.3.26 (2024-01-02)\n- Output:\n    - The Actor now stores `metadata.mimeType` for downloaded files (only applicable when `saveFiles` is enabled).\n\n## 0.3.25 (2023-12-21)\n- Input:\n    - Add `maxSessionRotations` input option for limiting the number of session rotations when recognized as a bot.\n- Behavior:\n    - Fail on `401`, `403` and `429` HTTP status codes.\n\n## 0.3.24 (2023-12-07)\n- Behavior:\n    - Fix a bug within the `expandClickableElements` utility function.\n\n## 0.3.23 (2023-12-06)\n- Behavior:\n    - Respect empty `<body>` tag when extracting text from HTML.\n    - Fix a bug with the `simplifiedBody === null` exception.\n\n## 0.3.22 (2023-12-04)\n- Input:\n    - `ignoreCanonicalUrl` toggle to deduplicate pages based on their actual URLs (useful when two different pages share the same canonical URL).\n- Behavior:\n    - Improve the large content detection - this fixes a regression from `0.3.21`.\n\n## 0.3.21 (2023-11-29)\n- Output:\n    - The `debug` mode now stores the results of all the extractors (+ raw HTML) as Key-Value Store objects.\n    - New extractor "Readable text with fallback" checks the results of the "Readable text" extractor and checks the content integrity on the fly.\n- Behavior:\n    - Skip text-cleaning and markdown processing step on large responses to avoid indefinite hangs.\n\n## 0.3.20 (2023-11-08)\n- Output:\n    - The `debug` mode now stores the raw page HTML (without the page preprocessing) under the `rawHtml` key.\n\n## 0.3.19 (2023-10-18)\n- Input:\n    - Add a default for `proxyConfiguration` option (which is now required since 0.3.18). This fixes the actor usage via API, falling back to the default proxy settings if they are not explicitly provided.\n\n## 0.3.18 (2023-10-17)\n- Input:\n    - Adds `includeUrlGlobs` option to allow explicit control over enqueuing logic (overrides the default scoping logic).\n    - Adds `requestTimeoutSecs` option to allow overriding the default request processing timeout.\n- Behavior:\n    - Disallow using large list of start URLs (more than 100) with deep crawling (`maxCrawlDepth > 0`) as it can lead to memory overflow.\n\n## 0.3.17 (2023-10-05)\n- Input:\n    - Adds `debugLog` option to enable debug logging.\n\n## 0.3.16 (2023-09-06)\n- Behavior:\n    - Raw HTTP Client (Cheerio) now works correctly with proxies again.\n\n## 0.3.15 (2023-08-30)\n- Input:\n    - `startUrls` is now a required input field.\n    - Input tooltips now provide more detailed description of crawler types and other input options.\n\n## 0.3.14 (2023-07-19)\n- Behavior:\n    - When using the Cheerio based crawlers, the actor processes links from removed elements correctly now.\n    - Crawlers now follow links in `<link>` tags (`rel=next, prev, help, search`).\n    - Relative canonical URLs are now getting correctly resolved during the deduplication phase.\n    - Actor now automatically recognizes blocked websites and retries the crawl with a new proxy / fingerprint combination.\n\n## 0.3.13 (2023-06-30)\n- Input:\n    - The Actor now uses new defaults for input settings for better user experience:\n        - default `crawlerType` is now `playwright:firefox`\n        - `saveMarkdown` is `true`\n        - `removeCookieWarnings` is `true`\n\n## 0.3.12 (2023-06-14)\n- Input:\n    - add `excludeUrlGlobs` for skipping certain URLs when enqueueing\n    - add `maxScrollHeightPixels` for scrolling down the page (useful for dynamically loaded pages)\n- Behavior:\n    - by default, the crawler scrolls down on every page to trigger dynamic loading (disable by setting `maxScrollHeightPixels` to 0)\n    - the crawler now handles HTML processing errors gracefully\n    - the actor now stays alive and restarts the crawl on certain known errors (Playwright Assertion Error).\n- Output:\n    - `crawl.httpStatusCode` now contains HTTP response status code.\n\n## 0.3.10 (2023-06-05)\n- Input:\n    - move `processedHtml` under `debug` object\n    - add `removeCookieWarnings` option to automatically remove cookie consent modals (via [I don\'t care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension)\n- Behavior:\n    - consider redirected start URL for prefix matching\n    - make URL deduplication case-insensitive\n    - wait at least 3s in playwright to let dynamic content to load\n    - retry the start URLs 10 times and regular URLs 5 times (to get around issues with retries on burnt proxies)\n    - ignore links not starting with `http`\n    - skip parsing non-html files\n    - support `startUrls` as text-file\n\n## 0.3.9 (2023-05-18)\n- Input:\n    - Updated README and input hints.\n\n## 0.3.8 (2023-05-17)\n- Input:\n    - `initialCookies` option for passing cookies to the crawler. Provide the cookies as a JSON array of objects with `"name"` and `"value"` keys. Example: `[{ "name": "token", "value": "123456" }]`.\n- Behavior:\n    - `textExtractor` option is now removed in favour of `htmlTransformer`\n    - `unfluff` extractor has been completely removed\n    - HTML is always simplified by removing some elements from it, those are configurable via `removeElementsCssSelector` option, which now defaults to a larger set of elements, including `<nav>`, `<footer>`, `<svg>`, and elements with `role` attribute set to one of `alert`, `banner`, `dialog`, `alertdialog`.\n    - New `htmlTransformer` option has been introduced which allows to configure how the simplified HTML is further processed. The output of this is still an HTML, which can be later used to generate markdown or plain text from it.\n    - The crawler will now try to expand collapsible sections automatically. This works by clicking on elements with `aria-expanded="false"` attribute. You can configure this selector via `clickElementsCssSelector` option.\n    - When using Playwright based crawlers, the dynamic content is awaited for based on network activity, rather than webpage changes. This should improve the reliability of the crawler.\n    - Firefox `SEC_ERROR_UNKNOWN_ISSUER` has been solved by preloading the recognized intermediate TLS certificates into the Docker image.\n    - Crawled URLs are retried in case their processing timeouts.\n\n## 0.3.7 (2023-05-10)\n- Behavior:\n    - URLs with redirects are now enqueued based on the original (unredirected) URL. This should prevent the actor from skipping relevant pages which are hidden behind redirects.\n    - The actor now considers all start URLs when enqueueing new links. This way, the user can specify multiple start URLs as a workaround for the actor skipping some relevant pages on the website.\n    - Error with not enqueueing URLs with certain query parameters is now fixed.\n- Output:\n    - The `.url` field now contains the main resource URL without the fragment (`#`) part.\n\n## 0.3.6 (2023-05-04)\n- Input:\n    - Made the `initialConcurrency` option visible in the input editor.\n    - Added `aggressivePruning` option. With this option set to `true`, the crawler will try to deduplicate the scraped content. This can be useful when the crawler is scraping a website with a lot of duplicate content (header menus, footers, etc.)\n- Behavior:\n    - The actor now stays alive and restarts the crawl on certain known errors (Playwright Assertion Error).\n\n## 0.3.4 (2023-05-04)\n- Input:\n   - Added a new hidden option `initialConcurrency`. This option sets the initial number of web browsers or HTTP clients running in parallel during the actor run. Increasing this number can speed up the crawling process. Bear in mind this option is hidden and can be changed only by editing the actor input using the JSON editor.\n\n## 0.3.3 (2023-04-28)\n- Input:\n   - Added a new option `maxResults` to limit the total number of results. If used with `maxCrawlPages`, the crawler will stop when either of the limits is reached.\n\n## 0.3.1 (2023-04-24)\n- Input:\n   - Added an option to download linked document files from the page - `saveFiles`. This is useful for downloading pdf, docx, xslx... files from the crawled pages. The files are saved to the default key-value store of the run and the links to the files are added to the dataset.\n   - Added a new crawler - Stealthy web browser - that uses a Firefox browser with a stealthy profile. It is useful for crawling websites that block scraping.\n\n## 0.0.13 (2023-04-18)\n- Input:\n    - Added new `textExtractor` option `readableText`. It is generally very accurate and has a good ratio of coverage to noise. It extracts only the main article body (similar to `unfluff`) but can work for more complex pages.\n    - Added `readableTextCharThreshold` option. This only applies to `readableText` extractor. It allows fine-tuning which part of the text should be focused on. That only matters for very complex pages where it is not obvious what should be extracted.\n- Output:\n    - Added simplified output view `Overview` that has only `url` and `text` for quick output check\n- Behavior:\n    - Domains starting with `www.` are now considered equal to ones without it. This means that the start URL `https://apify.com` can enqueue  `https://www.apify.com` and vice versa.\n\n## 0.0.10 (2023-04-05)\n- Input:\n    - Added new `crawlerType` option `jsdom` for processing with JSDOM. It allows client-side script processing, trying to mimic the browser behavior in Node.js but with much better performance. This is still experimental and may crash on some particular pages.\n    - Added `dynamicContentWaitSecs` option (defaults to 10s), which is the maximum waiting time for dynamic waiting.\n- Output (BREAKING CHANGE):\n    - Renamed `crawl.date` to `crawl.loadedTime`\n    - Moved `crawl.screenshotUrl` to top-level object\n    - The `markdown` field was made visible\n    - Renamed `metadata.language` to `metadata.languageCode`\n    - Removed `metadata.createdAt` (for now)\n    - Added `metadata.keywords`\n- Behavior:\n    - Added waiting for dynamically rendered content (supported in Headless browser and JSDOM crawlers). The crawler checks every half a second for content changes. When there are no changes for 2 seconds, the crawler proceeds to extraction.\n\n## 0.0.7 (2023-03-30)\n- Input:\n    - BREAKING CHANGE: Added `textExtractor` input option to choose how strictly to parse the content. Swapped the previous `unfluff` for `CrawleeHtmlToText` as default which in general will extract more text. We chose to output more text rather than less by default.\n    - Added `removeElementsCssSelector` which allows passing extra CSS selectors to further strip down the HTML before it is converted to text. This can help fine-tuning. By default, the actor removes the page navigation bar, header, and footer.\n- Output:\n    - Added markdown to output if `saveMarkdown` option is chosen\n    - All extractor outputs + HTML as a link can be obtained if `debugMode` is set.\n    - Added `pageType` to the output (only as `debug` for now), it will be fine-tuned in the future.\n- Behavior:\n    - Added deduplication by `canonicalUrl`. E.g. if more different URLs point to the same canonical URL, they are skipped\n    - Skip pages that redirect outside the original start URLs domain.\n    - Only run a single text extractor unless in debug mode. This improves performance.',
			},
			buildNumber: '0.3.67',
			usage: {
				ACTOR_COMPUTE_UNITS: 0.1607088888888889,
			},
			usageTotalUsd: 0.06428355555555557,
			usageUsd: {
				ACTOR_COMPUTE_UNITS: 0.06428355555555557,
			},
		},
	};
};

export const getDatasetItems = () => [
	{
		url: 'https://docs.apify.com/academy/web-scraping-for-beginners',
		crawl: {
			loadedUrl: 'https://docs.apify.com/academy/web-scraping-for-beginners',
			loadedTime: '2025-07-29T10:24:12.704Z',
			referrerUrl: 'https://docs.apify.com/academy/web-scraping-for-beginners',
			depth: 0,
			httpStatusCode: 200,
		},
		metadata: {
			canonicalUrl: 'https://docs.apify.com/academy/web-scraping-for-beginners',
			title: 'Web scraping basics for JavaScript devs | Academy | Apify Documentation',
			description:
				'Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.',
			author: null,
			keywords: null,
			languageCode: 'en',
			openGraph: [
				{
					property: 'og:url',
					content: 'https://docs.apify.com/academy/web-scraping-for-beginners',
				},
				{
					property: 'og:locale',
					content: 'en',
				},
				{
					property: 'og:title',
					content: 'Web scraping basics for JavaScript devs | Academy | Apify Documentation',
				},
				{
					property: 'og:description',
					content:
						'Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.',
				},
				{
					property: 'og:image',
					content:
						'https://apify.com/og-image/docs-article?title=Web+scraping+basics+for+JavaScript+devs',
				},
			],
			jsonLd: null,
			headers: {
				'content-type': 'text/html; charset=utf-8',
				'content-length': '9937',
				date: 'Tue, 29 Jul 2025 10:24:07 GMT',
				'x-fastly-request-id': '335284769a45f6663d6e781ae29e8de1e4955078',
				server: 'nginx',
				'x-origin-cache': 'HIT',
				'last-modified': 'Tue, 29 Jul 2025 09:09:43 GMT',
				'access-control-allow-origin': '*',
				'strict-transport-security': 'max-age=31556952',
				etag: 'W/"68888fd7-9290"',
				expires: 'Tue, 29 Jul 2025 10:17:25 GMT',
				'cache-control': 'max-age=600',
				'content-encoding': 'gzip',
				'x-proxy-cache': 'MISS',
				'x-github-request-id': '819E:17B773:1507DB9:16BDFA6:68889D5D',
				'accept-ranges': 'bytes',
				via: '1.1 varnish, 1.1 ea17beb3b7167ea4b16b5a6d11d59de4.cloudfront.net (CloudFront)',
				'x-served-by': 'cache-iad-kcgs7200137-IAD',
				'x-frame-options': 'SAMEORIGIN',
				'x-cache-hits': '0',
				'x-timer': 'S1753784648.747076, VS0, VE1',
				vary: 'Accept-Encoding',
				'x-cache': 'Miss from cloudfront',
				'x-amz-cf-pop': 'MAD56-P2',
				'x-amz-cf-id': 'jIe7mJ3Wj0GPP8UrKJer_vXNMPHV8ojQpJJ1IzhwchyeI1qpeyPF3A==',
				age: '464',
				'x-firefox-spdy': 'h2',
			},
		},
		screenshotUrl: null,
		text: "Web scraping basics for JavaScript devs | Academy\nLearn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.\nWelcome to Web scraping basics for JavaScript devs, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you're looking for a quick start, we recommend trying this tutorial instead.\nThis course is made by Apify, the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you'll be able to run your scrapers on any computer. No Apify account needed.\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the Apify platform course, where we'll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\nWhy learn scraper development?â€‹\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technologies like React or Next.js will be a piece of cake.\nCourse Summaryâ€‹\nWhen we set out to create the Academy, we wanted to build a complete guide to web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\nThis is what you'll learn in the Web scraping basics for JavaScript devs course:\nWeb scraping basics for JavaScript devs \nBasics of data extraction\nBasics of crawling\nBest practices\nRequirementsâ€‹\nYou don't need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don't be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you're new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using [] instead of () can make a lot of difference.\nIf you don't already have basic programming knowledge and would like to be well-prepared for this course, we recommend learning about JavaScript basics and CSS Selectors.\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\nIdeally, you should have at least a moderate understanding of the following concepts:\nJavaScript + Node.jsâ€‹\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and async...await), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\nasync...await (YouTube)\nJavaScript loops (MDN)\nModularity in Node.js\nGeneral web developmentâ€‹\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because their knowledge will be assumed (unless we're showing something out of the ordinary).\nHTML\nHTTP protocol\nDevTools\njQuery or Cheerioâ€‹\nWe'll be using the Cheerio package a lot to parse data from HTML. This package provides an API using jQuery syntax to help traverse downloaded HTML within Node.js.\nNext upâ€‹\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. Let's get to it!\nIf you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the Basics of crawling section.",
		markdown:
			'# Web scraping basics for JavaScript devs | Academy\n\n**Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.**\n\n* * *\n\nWelcome to **Web scraping basics for JavaScript devs**, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you\'re looking for a quick start, we recommend trying [this tutorial](https://blog.apify.com/web-scraping-javascript-nodejs/) instead.\n\nThis course is made by [Apify](https://apify.com/), the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you\'ll be able to run your scrapers on any computer. No Apify account needed.\n\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the [Apify platform course](https://docs.apify.com/academy/apify-platform), where we\'ll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\n\n## Why learn scraper development?[â€‹](#why-learn "Direct link to Why learn scraper development?")\n\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\n\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\n\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technologies like React or Next.js will be a piece of cake.\n\n## Course Summary[â€‹](#summary "Direct link to Course Summary")\n\nWhen we set out to create the Academy, we wanted to build a complete guide to web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\n\nThis is what you\'ll learn in the **Web scraping basics for JavaScript devs** course:\n\n*   [Web scraping basics for JavaScript devs](https://docs.apify.com/academy/web-scraping-for-beginners)\n    *   [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction)\n    *   [Basics of crawling](https://docs.apify.com/academy/web-scraping-for-beginners/crawling)\n    *   [Best practices](https://docs.apify.com/academy/web-scraping-for-beginners/best-practices)\n\n## Requirements[â€‹](#requirements "Direct link to Requirements")\n\nYou don\'t need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don\'t be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you\'re new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using `[]` instead of `()` can make a lot of difference.\n\n> If you don\'t already have basic programming knowledge and would like to be well-prepared for this course, we recommend learning about [JavaScript basics](https://developer.mozilla.org/en-US/curriculum/core/javascript-fundamentals/) and [CSS Selectors](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors).\n\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\n\nIdeally, you should have at least a moderate understanding of the following concepts:\n\n### JavaScript + Node.js[â€‹](#javascript-and-node "Direct link to JavaScript + Node.js")\n\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and `async...await`), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\n\n*   [`async...await` (YouTube)](https://www.youtube.com/watch?v=vn3tm0quoqE&ab_channel=Fireship)\n*   [JavaScript loops (MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Loops_and_iteration)\n*   [Modularity in Node.js](https://javascript.plainenglish.io/how-to-use-modular-patterns-in-nodejs-982f0e5c8f6e)\n\n### General web development[â€‹](#general-web-development "Direct link to General web development")\n\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because their knowledge will be **assumed** (unless we\'re showing something out of the ordinary).\n\n*   [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML)\n*   [HTTP protocol](https://developer.mozilla.org/en-US/docs/Web/HTTP)\n*   [DevTools](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools)\n\n### jQuery or Cheerio[â€‹](#jquery-or-cheerio "Direct link to jQuery or Cheerio")\n\nWe\'ll be using the [**Cheerio**](https://www.npmjs.com/package/cheerio) package a lot to parse data from HTML. This package provides an API using jQuery syntax to help traverse downloaded HTML within Node.js.\n\n## Next up[â€‹](#next "Direct link to Next up")\n\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. [Let\'s get to it!](https://docs.apify.com/academy/web-scraping-for-beginners/introduction)\n\n> If you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the [Basics of crawling](https://docs.apify.com/academy/web-scraping-for-beginners/crawling) section.',
		debug: {
			requestHandlerMode: 'browser',
		},
	},
	{
		url: 'https://docs.apify.com/academy/web-scraping-for-beginners/best-practices',
		crawl: {
			loadedUrl: 'https://docs.apify.com/academy/web-scraping-for-beginners/best-practices',
			loadedTime: '2025-07-29T10:33:50.212Z',
			referrerUrl: 'https://docs.apify.com/academy/web-scraping-for-beginners',
			depth: 1,
			httpStatusCode: 200,
		},
		metadata: {
			canonicalUrl: 'https://docs.apify.com/academy/web-scraping-for-beginners/best-practices',
			title: 'Best practices | Academy | Apify Documentation',
			description:
				'Understand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.',
			author: null,
			keywords: null,
			languageCode: 'en',
			openGraph: [
				{
					property: 'og:url',
					content: 'https://docs.apify.com/academy/web-scraping-for-beginners/best-practices',
				},
				{
					property: 'og:locale',
					content: 'en',
				},
				{
					property: 'og:title',
					content: 'Best practices | Academy | Apify Documentation',
				},
				{
					property: 'og:description',
					content:
						'Understand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.',
				},
				{
					property: 'og:image',
					content: 'https://apify.com/og-image/docs-article?title=Best+practices',
				},
			],
			jsonLd: null,
			headers: {
				'content-type': 'text/html; charset=utf-8',
				'content-length': '11392',
				date: 'Tue, 29 Jul 2025 10:33:42 GMT',
				'x-fastly-request-id': '33c27eccfad5fa20a114e4c57e92cd7326656fa4',
				server: 'nginx',
				'x-origin-cache': 'HIT',
				'last-modified': 'Tue, 29 Jul 2025 09:09:43 GMT',
				'access-control-allow-origin': '*',
				'strict-transport-security': 'max-age=31556952',
				etag: 'W/"68888fd7-b4a4"',
				expires: 'Tue, 29 Jul 2025 10:28:58 GMT',
				'cache-control': 'max-age=600',
				'content-encoding': 'gzip',
				'x-proxy-cache': 'MISS',
				'x-github-request-id': 'DC24:13A37C:154749E:1665B61:6888A010',
				'accept-ranges': 'bytes',
				via: '1.1 varnish, 1.1 36dcf1a6ec983195b309a349ed6dd758.cloudfront.net (CloudFront)',
				'x-served-by': 'cache-iad-kcgs7200161-IAD',
				'x-frame-options': 'SAMEORIGIN',
				'x-cache-hits': '0',
				'x-timer': 'S1753785222.313528, VS0, VE10',
				vary: 'Accept-Encoding',
				'x-cache': 'Miss from cloudfront',
				'x-amz-cf-pop': 'MAD56-P2',
				'x-amz-cf-id': '4p1J2VJWxZeIFTlltnq8ShYmifR-M1EDqXy-WXSQtWLSUG-9Qp6x9w==',
				age: '0',
				'x-firefox-spdy': 'h2',
			},
		},
		screenshotUrl: null,
		text: "Best practices | Academy | Apify Documentation\nBest practices when writing scrapers\nUnderstand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.\nEvery developer has their own style, which evolves as they grow and learn. While one dev might prefer a more functional style, another might find an imperative approach to be more intuitive. We at Apify understand this, and have written this best practices lesson with that in mind.\nThe goal of this lesson is not to force you into a specific paradigm or to make you think that you're doing things wrong, but instead to provide you some insight into the standards and best practices that we at Apify follow to ensure readable, maintainable, scalable code.\nCode styleâ€‹\nWhen it comes to your code style when writing scrapers, there are some general things we recommend.\nClean codeâ€‹\nPraise clean code! Use proper variable and function names that are descriptive of what they are, and split your code into smaller pure functions.\nConstant variablesâ€‹\nDefine any constant variables that globally apply to the scraper in a single file named constants.js, from where they will all be imported. Constant variable names should be in UPPERCASE_WITH_UNDERSCORES style.\nIf you have a whole lot of constant variables, they can be in a folder named constants organized into different files.\nUse ES6 JavaScriptâ€‹\nIf you're writing your scraper in JavaScript, use ES6 features and ditch the old ones which they replace. This means using const and let instead of var, includes instead of indexOf, etc.\nTo learn more about some of the most popular (and awesome) ES6+ features, check out this article.\nNo magic numbersâ€‹\nAvoid using magic numbers as much as possible. Either declare them as a constant variable in your constants.js file, or if they are only used once, add a comment explaining what the number is.\nDon't write code like this:\nconst x = (y) => (y - 32) * (5 / 9);\nThat is quite confusing due to the nondescriptive naming and the magic numbers. Do this instead:\n// Converts a fahrenheit value to celsius\nconst fahrenheitToCelsius = (celsius) => (celsius - 32) * (5 / 9);\nDon't be shy to add comments to your code! Even when using descriptive function and variable naming, it might still be a good idea to add a comment in places where you had to make a tough decision or chose an unusual choice.\nIf you're a true pro, use JSDoc to comment and document your code.\nLoggingâ€‹\nLogging helps you understand exactly what your scraper is doing. Generally, having more logs is better than having fewer. Especially make sure to log your catch blocks - no error should pass unseen unless there is a good reason.\nFor scrapers that will run longer than usual, keep track of some useful stats (such as itemsScraped or errorsHit) and log them to the console on an interval.\nThe meaning of your log messages should make sense to an outsider who is not familiar with the inner workings of your scraper. Avoid log lines with just numbers or just URLs - always identify what the number/string means.\nHere is an example of an \"incorrect\" log message:\n300 https://example.com/1234 1234\nAnd here is that log message translated into something that makes much more sense to the end user:\nIndex 1234 --- https://example.com/1234 --- took 300 ms\nInputâ€‹\nWhen it comes to accepting input into a scraper, two main best practices should be followed.\nSet limitsâ€‹\nWhen allowing your users to pass input properties which could break the scraper (such as timeout set to 0), be sure to disallow ridiculous values. Set a maximum/minimum number allowed, maximum array input length, etc.\nValidateâ€‹\nValidate the input provided by the user! This should be the very first thing your scraper does. If the fields in the input are missing or in an incorrect type/format, either parse the value and correct it programmatically or throw an informative error telling the user how to fix the error.\nOn the Apify platform, you can use the input schema to both validate inputs and generate a clean UI for those using your scraper.\nError handlingâ€‹\nErrors are bound to occur in scrapers. Perhaps it got blocked, or perhaps the data scraped was corrupted in some way.\nWhatever the reason, a scraper shouldn't completely crash when an error occurs. Use try...catch blocks to catch errors and log useful messages. The log messages should indicate where the error happened, and what type of error happened.\nBad error log message:\nCannot read property â€œ0â€ from undefined\nGood error log message:\nCould not parse an address, skipping the page. Url: https://www.example-website.com/people/1234\nThis doesn't mean that you should absolutely litter your code with try...catch blocks, but it does mean that they should be placed in error-prone areas (such as API calls or testing a string with a specific regular expression).\nIf the error that has occurred renders that run of the scraper completely useless, exit the process immediately.\nLogging is the minimum you should be doing though. For example, if you have an entire object of scraped data and just the price field fails to be parsed, you might not want to throw away the rest of that data. Rather, it could still be pushed to the output and a log message like this could appear:\nWe could not parse the price of product: Men's Trainers Orange, pushing anyways.\nThis really depends on your use case though. If you want 100% clean data, you might not want to push incomplete objects and just retry (ideally) or log an error message instead.\nRecapâ€‹\nWow, that's a whole lot of things to abide by! How will you remember all of them? Try to follow these three points:\nDescribe your code as you write it with good naming, constants, and comments. It should read like a book.\nAdd log messages at points throughout your code so that when it's running, you (and everyone else) know what's going on.\nHandle errors appropriately. Log the error and either retry, or continue on. Only throw if the error will be caught or if the error is absolutely detrimental to the scraper's run.",
		markdown:
			'# Best practices | Academy | Apify Documentation\n\n## Best practices when writing scrapers\n\n**Understand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.**\n\n* * *\n\nEvery developer has their own style, which evolves as they grow and learn. While one dev might prefer a more [functional](https://en.wikipedia.org/wiki/Functional_programming) style, another might find an [imperative](https://en.wikipedia.org/wiki/Imperative_programming) approach to be more intuitive. We at Apify understand this, and have written this best practices lesson with that in mind.\n\nThe goal of this lesson is not to force you into a specific paradigm or to make you think that you\'re doing things wrong, but instead to provide you some insight into the standards and best practices that we at Apify follow to ensure readable, maintainable, scalable code.\n\n## Code style[â€‹](#code-style "Direct link to Code style")\n\nWhen it comes to your code style when writing scrapers, there are some general things we recommend.\n\n### Clean code[â€‹](#clean-code "Direct link to Clean code")\n\nPraise [clean code](https://blog.risingstack.com/javascript-clean-coding-best-practices-node-js-at-scale/)! Use proper variable and function names that are descriptive of what they are, and split your code into smaller [pure](https://en.wikipedia.org/wiki/Pure_function) functions.\n\n### Constant variables[â€‹](#constants "Direct link to Constant variables")\n\nDefine any [constant variables](https://softwareengineering.stackexchange.com/questions/250619/best-practices-reasons-for-string-constants-in-javascript) that globally apply to the scraper in a single file named **constants.js**, from where they will all be imported. Constant variable names should be in `UPPERCASE_WITH_UNDERSCORES` style.\n\n> If you have a whole lot of constant variables, they can be in a folder named **constants** organized into different files.\n\n### Use ES6 JavaScript[â€‹](#use-es6 "Direct link to Use ES6 JavaScript")\n\nIf you\'re writing your scraper in JavaScript, use [ES6](https://www.w3schools.com/js/js_es6.asp) features and ditch the old ones which they replace. This means using `const` and `let` instead of `var`, `includes` instead of `indexOf`, etc.\n\n> To learn more about some of the most popular (and awesome) ES6+ features, check out [this](https://medium.com/@matthiasvstephens/why-is-es6-so-awesome-88bff6857849) article.\n\n### No magic numbers[â€‹](#no-magic-numbers "Direct link to No magic numbers")\n\nAvoid using [magic numbers](https://en.wikipedia.org/wiki/Magic_number_\\(programming\\)) as much as possible. Either declare them as a **constant** variable in your **constants.js** file, or if they are only used once, add a comment explaining what the number is.\n\nDon\'t write code like this:\n\n```\nconst x = (y) => (y - 32) * (5 / 9);\n```\n\nThat is quite confusing due to the nondescriptive naming and the magic numbers. Do this instead:\n\n```\n// Converts a fahrenheit value to celsiusconst fahrenheitToCelsius = (celsius) => (celsius - 32) * (5 / 9);\n```\n\nDon\'t be shy to add comments to your code! Even when using descriptive function and variable naming, it might still be a good idea to add a comment in places where you had to make a tough decision or chose an unusual choice.\n\n> If you\'re a true pro, use [JSDoc](https://jsdoc.app/) to comment and document your code.\n\n## Logging[â€‹](#logging "Direct link to Logging")\n\nLogging helps you understand exactly what your scraper is doing. Generally, having more logs is better than having fewer. Especially make sure to log your `catch` blocks - no error should pass unseen unless there is a good reason.\n\nFor scrapers that will run longer than usual, keep track of some useful stats (such as **itemsScraped** or **errorsHit**) and log them to the console on an interval.\n\nThe meaning of your log messages should make sense to an outsider who is not familiar with the inner workings of your scraper. Avoid log lines with just numbers or just URLs - always identify what the number/string means.\n\nHere is an example of an "incorrect" log message:\n\n```\n300  https://example.com/1234  1234\n```\n\nAnd here is that log message translated into something that makes much more sense to the end user:\n\n```\nIndex 1234 --- https://example.com/1234 --- took 300 ms\n```\n\n## Input[â€‹](#input "Direct link to Input")\n\nWhen it comes to accepting input into a scraper, two main best practices should be followed.\n\n### Set limits[â€‹](#set-limits "Direct link to Set limits")\n\nWhen allowing your users to pass input properties which could break the scraper (such as **timeout** set to **0**), be sure to disallow ridiculous values. Set a maximum/minimum number allowed, maximum array input length, etc.\n\n### Validate[â€‹](#validate "Direct link to Validate")\n\nValidate the input provided by the user! This should be the very first thing your scraper does. If the fields in the input are missing or in an incorrect type/format, either parse the value and correct it programmatically or throw an informative error telling the user how to fix the error.\n\n> On the Apify platform, you can use the [input schema](https://docs.apify.com/academy/deploying-your-code/input-schema) to both validate inputs and generate a clean UI for those using your scraper.\n\n## Error handling[â€‹](#error-handling "Direct link to Error handling")\n\nErrors are bound to occur in scrapers. Perhaps it got blocked, or perhaps the data scraped was corrupted in some way.\n\nWhatever the reason, a scraper shouldn\'t completely crash when an error occurs. Use `try...catch` blocks to catch errors and log useful messages. The log messages should indicate where the error happened, and what type of error happened.\n\nBad error log message:\n\n```\nCannot read property â€œ0â€ from undefined\n```\n\nGood error log message:\n\n```\nCould not parse an address, skipping the page. Url: https://www.example-website.com/people/1234\n```\n\nThis doesn\'t mean that you should absolutely litter your code with `try...catch` blocks, but it does mean that they should be placed in error-prone areas (such as API calls or testing a string with a specific regular expression).\n\n> If the error that has occurred renders that run of the scraper completely useless, exit the process immediately.\n\nLogging is the minimum you should be doing though. For example, if you have an entire object of scraped data and just the **price** field fails to be parsed, you might not want to throw away the rest of that data. Rather, it could still be pushed to the output and a log message like this could appear:\n\n```\nWe could not parse the price of product: Men\'s Trainers Orange, pushing anyways.\n```\n\nThis really depends on your use case though. If you want 100% clean data, you might not want to push incomplete objects and just retry (ideally) or log an error message instead.\n\n## Recap[â€‹](#recap "Direct link to Recap")\n\nWow, that\'s a whole lot of things to abide by! How will you remember all of them? Try to follow these three points:\n\n1.  Describe your code as you write it with good naming, constants, and comments. It **should read like a book**.\n2.  Add log messages at points throughout your code so that when it\'s running, you (and everyone else) know what\'s going on.\n3.  Handle errors appropriately. Log the error and either retry, or continue on. Only throw if the error will be caught or if the error is absolutely detrimental to the scraper\'s run.',
		debug: {
			requestHandlerMode: 'browser',
		},
	},
];
